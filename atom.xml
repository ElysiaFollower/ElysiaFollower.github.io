<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Seele</title>
  
  <subtitle>subtitle</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2025-09-02T16:19:32.450Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Prometheus17</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>欢迎来到AIGC的魔法世界</title>
    <link href="http://example.com/2025/09/02/%E6%AC%A2%E8%BF%8E%E6%9D%A5%E5%88%B0AIGC%E7%9A%84%E9%AD%94%E6%B3%95%E4%B8%96%E7%95%8C/"/>
    <id>http://example.com/2025/09/02/%E6%AC%A2%E8%BF%8E%E6%9D%A5%E5%88%B0AIGC%E7%9A%84%E9%AD%94%E6%B3%95%E4%B8%96%E7%95%8C/</id>
    <published>2025-09-02T15:48:03.000Z</published>
    <updated>2025-09-02T16:19:32.450Z</updated>
    
    <content type="html"><![CDATA[<p>参考在线课程<a class="link" href="https://modelscope.cn/learn/1582?pid=1577">https://modelscope.cn/learn/1582?pid=1577<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a> <sup id="fnref:1_0"><a class="link" href="javascript:void(0);" rel="footnote" onclick="footnote_rememberSource('1', '1_0'); footnote_jumpToAnchor('fn:1')">[1]<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></sup></p><p>特别鸣谢: 阿里魔搭ModelScope平台</p><p>本文旨在对课程核心内容进行记录与提炼，并融入了一些个人的理解和思考。文中若有任何疏漏或理解不当之处，敬请各位读者不吝指正，共同交流探讨(^_−)☆</p><ul><li>鉴于文本生成技术已广为人知，本文将重点聚焦于当前发展迅速且形式新颖的<strong>非文本内容生成</strong>，主要涵盖<strong>图像、音频及视频生成</strong>三大领域。</li></ul><p>侵删</p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><h2 id="生成模型的演化历程"><a href="#生成模型的演化历程" class="headerlink" title="生成模型的演化历程"></a>生成模型的演化历程</h2><h3 id="生成模型的本质是什么？"><a href="#生成模型的本质是什么？" class="headerlink" title="生成模型的本质是什么？"></a>生成模型的本质是什么？</h3><p><img lazyload="" src="/images/loading.svg" data-src="https://pub-a1cb67b2f5c34421bbd8c98bcf68643b.r2.dev/img/2025/09/02/%25E7%2594%259F%25E6%2588%2590%25E6%25A8%25A1%25E5%259E%258B%25E6%259C%25AC%25E8%25B4%25A8%25E5%259B%25BE_2025-09-02.png" alt="生成模型本质图"></p><p>生成模型的本质，在于将一个服从<strong>简单随机分布(如高斯分布)的噪声</strong>与一个<strong>引导条件</strong>作为输入，通过模型复杂的非线性映射，将其转化为一个服从**目标数据真实分布(复杂随机分布)**的输出，这些输出最终表现为我们所见的图像、音频或视频等形式。</p><ul><li><p><strong>随机噪声</strong>：</p><ul><li>它是生成多样性的来源。对于一个确定的模型，如果仅仅输入固定的引导条件(如提示词)， 理论上其输出应该是恒定的，因为模型本身是确定性的张量运算<del class="mask">(就算是dropout层，也只是在训练的时候其作用，不会影响测试时的确定性)</del>。然而在实践中，即使用户输入完全相同的提示词，每次生成的结果却也各不相同。这是因为模型在每次生成时，都会引入一个用户无法直接感知的随机噪声(通常由随机种子<code>seed</code>控制)。这个初始噪声的微小差异，经过模型的放大和转换，最终导致了输出结果的多样性。</li></ul></li><li><p><strong>引导条件</strong>：</p><ul><li>这是用户可以控制的，用于指导生成方向的输入，例如<strong>提示词</strong>、**参考图像 (如线稿) **等</li></ul></li></ul><p><strong>生成式模型vs检索系统</strong>：</p><p><img lazyload="" src="/images/loading.svg" data-src="https://pub-a1cb67b2f5c34421bbd8c98bcf68643b.r2.dev/img/2025/09/03/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8Bvs%E6%A3%80%E7%B4%A2%E7%B3%BB%E7%BB%9F_2025-09-02.png" alt="生成模型vs检索系统"></p><p>从概念上讲，生成式模型与检索系统存在一个有趣的类比。两者都能根据用户输入的引导条件 (如文本) 返回相关的图像。<strong>它们的核心区别在于输出空间的性质</strong>：检索系统的输出空间是<strong>离散的</strong> (从有限的数据中选取，是空间中离散的点)， 而生成模型的输出空间是<strong>连续的</strong>。</p><p>这种连续性正是基于一个核心理念：<strong>与现代NLP模型类似，先进的图像生成模型同样在名为 “潜在空间(Latent Space)” 的高维向量空间中运作</strong>。 无论是输入的文本提示词，还是生成过程中的图像特征，都会被编码为这个空间中的向量(Embedding)。在这个空间中，语义相近的概念在空间位置上也相互靠近。<del class="mask">关于潜在向量空间的理解，最早可以追溯到Word2Vec的词向量，可见这篇文章的解释</del></p><p>因此，生成模型可以被视为一种<strong>平滑的检索</strong>。举个例子：它不仅能够找到代表<strong>程序员</strong>和<strong>猩猩</strong>的向量区域，更能在这些区域之间进行插值，从而创造出一个在训练数据中从未出现过，但语义上融合了两者特征的全新向量，最终解码为**程序“猿”**的图像。而离散的检索系统只能输出向量数据库中已有的离散点。</p><h4 id="生成式模型的演变"><a href="#生成式模型的演变" class="headerlink" title="生成式模型的演变"></a>生成式模型的演变</h4><p><img lazyload="" src="/images/loading.svg" data-src="https://pub-a1cb67b2f5c34421bbd8c98bcf68643b.r2.dev/img/2025/09/03/%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%BC%94%E5%8F%98_2025-09-02.png" alt="生成式模型的演变"></p><p>如果要深入学习这个领域的话，或许可以参考这个时间脉络去进行学习。<del class="mask">毕竟论文作者很多时候都默认读者知道一些前置的模型或概念，若是不提前了解可能会在理解上遇到一些困难</del></p><ul><li>当前主流的模型为 <strong>扩散(Diffusion)模型</strong> <ul><li>其基本原理可以直观地理解为：从一张纯粹的噪声图开始，多步迭代，逐步去噪，在引导条件的指引下，最终“还原”出一张清晰的图像</li><li>因为去噪过程实际主要依赖的是<strong>对噪声的预测能力</strong>，所以训练过程与生成过程恰好相反：向真实图像逐步叠加噪声，让模型在每一步都学习如何预测并移除被添加的噪声</li></ul></li></ul><h1 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h1><p>如果手上缺少算力的话可以在魔搭社区的AIGC专区玩玩 <a class="link" href="https://modelscope.cn/aigc/home">https://modelscope.cn/aigc/home<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p>如果有算力并且比较爱折腾的话，可以考虑使用魔搭团队开源的<strong>DiffSynth-Studio</strong>引擎<sup id="fnref:2_0"><a class="link" href="javascript:void(0);" rel="footnote" onclick="footnote_rememberSource('2', '2_0'); footnote_jumpToAnchor('fn:2')">[2]<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></sup></p><h2 id="模型推理"><a href="#模型推理" class="headerlink" title="模型推理"></a>模型推理</h2><h4 id="文生图"><a href="#文生图" class="headerlink" title="文生图"></a>文生图</h4><p>文生图是最基本的生图模式，也是应用最简单，普及最广泛的一种生成模式。</p><p>只需要用户输入一段文本形式的提示词，模型就能够生成一张相关的图像。</p><h5 id="提示词写作技巧"><a href="#提示词写作技巧" class="headerlink" title="提示词写作技巧"></a>提示词写作技巧</h5><p>前面我们已经提到，用户输入的提示实际上是对模型的一种<strong>引导条件</strong>，所以很多时候我们需要通过提示词对图像生成的内容进行引导，提示词的质量直接或间接地决定了生成图像的质量。容易理解，为了更好地指导模型，我们可能需要一些<strong>摄影的技巧</strong>；可能需要一些<strong>构图的知识</strong>   <del class="mask">不过这大抵上属于是锦上添花，即便不懂摄影或构图，只要有审美细胞，都能进行调整</del></p><p>关于写作提示词的具体技巧(范式)有</p><ul><li>提示词反推 —— 已经有了一张图，可以在这张图的基础上反推出提示词</li><li>提示词润色 —— 更精细的描述</li><li>负向提示词 —— 不要什么</li><li>翻译<ul><li><del class="mask">因为有些图像模型，并不支持中文输入</del>，但实际上翻译这一步有些时候可以省去。虽然很多时候使用英文提示词往往比使用中文提示词表现更好，但是我非常认可一个观点——<strong>概念优于语言。与其纠结prompt的语言，不如思考如何对AI描述清楚你的需求</strong> <sup id="fnref:3_0"><a class="link" href="javascript:void(0);" rel="footnote" onclick="footnote_rememberSource('3', '3_0'); footnote_jumpToAnchor('fn:3')">[3]<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></sup></li></ul></li></ul><p>但事实上，我们可以<strong>让大语言模型来生成提示词</strong>，然后再自行调整</p><p>可以参考的工具有很多<sup id="fnref:4_0"><a class="link" href="javascript:void(0);" rel="footnote" onclick="footnote_rememberSource('4', '4_0'); footnote_jumpToAnchor('fn:4')">[4]<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></sup>：</p><ul><li><p><strong>Learning Prompt</strong>: 老是全权依赖工具也不是回事，可以在这里<strong>学些提示词设计经验</strong></p></li><li><p><strong>PromptPort</strong>: 有很多现成的Prompt模版可以参考交流</p><ul><li>FlowGPT已经转型应用商店了，不太好搞。但是PromptPort体验还不错，提示词模版涉及各个领域，而且基本都有中英两版，<del class="mask">可以阅读中文版快速理解然后复制英文版使用</del></li></ul></li><li><p><strong>魔咒百科词典</strong>：魔法导论工具, 简单易用的AI绘画tag生成器</p><ul><li>在下第一次接触AI绘画的时候就听到有人喜欢使用“<strong>魔法导论</strong>”这样的说法来指代这个技术，这个比喻实在是有趣又贴切，因为基本的文生图AI绘画的的确确给人一种魔法的感觉，尤其是普通人第一次接触。只需要念出一连串咒语<del class="mask">(关键词提示词)</del>，再经由神秘的不可知域<del class="mask">（模型黑箱）</del>赐福，就能够创造出令人惊叹的“无上奇迹”<del class="mask">（让一个普通人在极短的时间内创造出精密的图像）</del>。同时它也似魔法一般，时而成功时而失败，不可预测。</li><li>而这份“魔咒百科词典”，就是在这个魔法语境下的 <strong>提示词</strong> 模版啦。只是AI绘画的提示词往往都是关键词的形式存在的，所以正巧就变成了魔咒词典。</li></ul></li></ul><h4 id="可控生成技术"><a href="#可控生成技术" class="headerlink" title="可控生成技术"></a>可控生成技术</h4><p>除了提示词这种引导形式外，还有很多其他形式的引导条件，如图所示</p><p><img lazyload="" src="/images/loading.svg" data-src="https://pub-a1cb67b2f5c34421bbd8c98bcf68643b.r2.dev/img/2025/09/03/%E5%8F%AF%E6%8E%A7%E7%94%9F%E6%88%90%E6%8A%80%E6%9C%AF_2025-09-02.png" alt="可控生成技术"></p><p>可控生成技术为生成过程提供了远超文本提示词的、精细化的空间和结构控制，在实践中已得到广泛应用，其中非常具有代表性的框架就是<strong>ControlNet</strong> <sup id="fnref:5_0"><a class="link" href="javascript:void(0);" rel="footnote" onclick="footnote_rememberSource('5', '5_0'); footnote_jumpToAnchor('fn:5')">[5]<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></sup></p><p>ControlNet及其衍生技术，允许用户输入一张控制图(如线稿、深度图、人体姿态骨骼图等)，<strong>在保持原图结构、姿态或构图的基础上，由扩散模型进行内容的重绘和填充</strong>。</p><p>不难想象，这是在生成领域至关重要的一项技术。因为人类多数时候难以仅用自然语言把需求完美地进行传递。在这项技术出现前，仅靠提示词进行图像生成，就像是在开盲盒，充满了随机性。</p><p>例如在ComfyUI这类<strong>基于节点的工作流编排工具中</strong>，上述可控生成技术就是核心组成部分。一个节点的输出（如姿态检测结果）可以作为下一个生成节点的控制输入，从而实现复杂的多步生成任务。</p><p>ps: 从某种意义上来讲LoRA或许也算得上是一种控制，只不过是属于<strong>风格、角色或概念层面的控制</strong>，而非<strong>空间结构控制</strong>。</p><h4 id="图像融合技术"><a href="#图像融合技术" class="headerlink" title="图像融合技术"></a>图像融合技术</h4><p><img lazyload="" src="/images/loading.svg" data-src="https://pub-a1cb67b2f5c34421bbd8c98bcf68643b.r2.dev/img/2025/09/03/%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88%E6%8A%80%E6%9C%AF-20250902.png" alt="图像融合技术"></p><p>试试融合一个篮球和一只鸡会怎么样(x</p><h4 id="工作流式应用搭建"><a href="#工作流式应用搭建" class="headerlink" title="工作流式应用搭建"></a>工作流式应用搭建</h4><p><img lazyload="" src="/images/loading.svg" data-src="https://pub-a1cb67b2f5c34421bbd8c98bcf68643b.r2.dev/img/2025/09/03/%E5%B7%A5%E4%BD%9C%E6%B5%81%E5%BA%94%E7%94%A8-20250902.png" alt="工作流应用"></p><p>想象一下现实中画师的工作，大多数画师也不会一笔成稿，而是经历线稿到上色，人物形体到饰品等过程。让AI也像人类一样每次专注于一个任务可以取得更好的效果。</p><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><h4 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h4><p>全称<strong>Low Rank Adaptation</strong></p><p>一种在预训练的图像生成模型训练完成后，<strong>低成本拓展生成能力</strong>的技术<sup id="fnref:6_0"><a class="link" href="javascript:void(0);" rel="footnote" onclick="footnote_rememberSource('6', '6_0'); footnote_jumpToAnchor('fn:6')">[6]<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></sup><sup id="fnref:7_0"><a class="link" href="javascript:void(0);" rel="footnote" onclick="footnote_rememberSource('7', '7_0'); footnote_jumpToAnchor('fn:7')">[7]<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></sup></p><p><img lazyload="" src="/images/loading.svg" data-src="https://pub-a1cb67b2f5c34421bbd8c98bcf68643b.r2.dev/img/2025/09/03/LoRA%E5%8E%9F%E7%90%86%E5%9B%BE-20250902.png" alt="LoRA原理图"></p><p>左侧蓝色矩阵为预训练模型原先训练好的参数矩阵<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex;" xmlns="http://www.w3.org/2000/svg" width="3.123ex" height="1.92ex" role="img" focusable="false" viewBox="0 -683 1380.6 848.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></g></svg></mjx-container>，这个矩阵相对来说较为庞大(秩比较大)，如果直接进行微调，将会导致需要修改的参数量很大，训练量非常大。<br>于是我们可以增加两个小矩阵<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="4.42ex" height="2.059ex" role="img" focusable="false" viewBox="0 -716 1953.7 910"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(750,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1194.7,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g></g></g></svg></mjx-container>，即图中右侧橘黄色的两个小矩阵，它们是分别是一个横长条的矩阵和一个竖长条的矩阵。假设原矩阵<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex;" xmlns="http://www.w3.org/2000/svg" width="3.123ex" height="1.92ex" role="img" focusable="false" viewBox="0 -683 1380.6 848.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></g></svg></mjx-container>大小为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="5.119ex" height="1.593ex" role="img" focusable="false" viewBox="0 -694 2262.4 704"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(742.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1742.4,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></svg></mjx-container> （<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="5.121ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 2263.4 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(742.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1742.4,0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></svg></mjx-container> 同理）, 那么这两个小矩阵就是 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="4.963ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 2193.4 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(742.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1742.4,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="4.963ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 2193.4 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(673.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1673.4,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></svg></mjx-container>， 于是两个小矩阵可以相乘消去r，得到一个和预训练参数矩阵维度完全一致的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="5.119ex" height="1.593ex" role="img" focusable="false" viewBox="0 -694 2262.4 704"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(742.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1742.4,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></svg></mjx-container> 的矩阵<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="4.256ex" height="1.67ex" role="img" focusable="false" viewBox="0 -716 1881 738"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="394" d="M51 0Q46 4 46 7Q46 9 215 357T388 709Q391 716 416 716Q439 716 444 709Q447 705 616 357T786 7Q786 4 781 0H51ZM507 344L384 596L137 92L383 91H630Q630 93 507 344Z"></path></g><g data-mml-node="mi" transform="translate(833,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g></g></g></svg></mjx-container>, 在进行预测的时候只需要将<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex;" xmlns="http://www.w3.org/2000/svg" width="10.145ex" height="1.994ex" role="img" focusable="false" viewBox="0 -716 4484 881.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g><g data-mml-node="mo" transform="translate(1602.8,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(2603,0)"><path data-c="394" d="M51 0Q46 4 46 7Q46 9 215 357T388 709Q391 716 416 716Q439 716 444 709Q447 705 616 357T786 7Q786 4 781 0H51ZM507 344L384 596L137 92L383 91H630Q630 93 507 344Z"></path></g><g data-mml-node="mi" transform="translate(3436,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g></g></g></svg></mjx-container> 作为新的模型权重参数，就可以实现对预训练参数的调整啦~</p><p><img lazyload="" src="/images/loading.svg" data-src="https://pub-a1cb67b2f5c34421bbd8c98bcf68643b.r2.dev/img/2025/09/03/LoRA%E5%8E%9F%E7%90%86%E5%85%AC%E5%BC%8F-20250902.png" alt="LoRA原理公式"></p><ul><li>ps: 在实际应用场景中，往往还会给增量矩阵<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="4.256ex" height="1.67ex" role="img" focusable="false" viewBox="0 -716 1881 738"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="394" d="M51 0Q46 4 46 7Q46 9 215 357T388 709Q391 716 416 716Q439 716 444 709Q447 705 616 357T786 7Q786 4 781 0H51ZM507 344L384 596L137 92L383 91H630Q630 93 507 344Z"></path></g><g data-mml-node="mi" transform="translate(833,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g></g></g></svg></mjx-container>先乘上一个缩放因子<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.448ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 640 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z"></path></g></g></g></svg></mjx-container>, 从而控制LoRA的力道。</li></ul><p>因为r要远小于d，所以两个小矩阵的参数量<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="8.859ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 3915.9 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mo" transform="translate(722.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1722.4,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(2464.7,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(3464.9,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container> 就要比<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="5.119ex" height="1.593ex" role="img" focusable="false" viewBox="0 -694 2262.4 704"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(742.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1742.4,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></svg></mjx-container> 少上很多，训练时要调整的参数量也少不少，故而使得训练成本大大降低。</p><p>图中B=0是为了保证在训练开始的时候，能够沿用原模型的能力, 因为训练开始的时候，BA为零矩阵，加到<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex;" xmlns="http://www.w3.org/2000/svg" width="3.123ex" height="1.92ex" role="img" focusable="false" viewBox="0 -683 1380.6 848.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></g></svg></mjx-container>上不会发生任何事。</p><p>但是注意，此时A绝不能为0！</p><ul><li>试想一下A和B同时为0会发生什么，梯度会彻底消失！简单想象一下，有一个表达式 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="11.455ex" height="1.783ex" role="img" focusable="false" viewBox="0 -583 5063.1 788"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(767.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(1823.6,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(3054.3,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="msub" transform="translate(4054.6,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container>​,  如果<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.282ex" height="1.339ex" role="img" focusable="false" viewBox="0 -442 1008.6 592"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container> 为0，那么对<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.282ex" height="1.339ex" role="img" focusable="false" viewBox="0 -442 1008.6 592"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container>求导的时候，导数为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.282ex" height="1.339ex" role="img" focusable="false" viewBox="0 -442 1008.6 592"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container>，那么只要<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.282ex" height="1.339ex" role="img" focusable="false" viewBox="0 -442 1008.6 592"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container>不为0，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.282ex" height="1.339ex" role="img" focusable="false" viewBox="0 -442 1008.6 592"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container>就有梯度，就有可能可以训练变化；但如果此时<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.282ex" height="1.339ex" role="img" focusable="false" viewBox="0 -442 1008.6 592"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container>为0，则梯度为0，这个参数完全不参与最后的预测，也自然不会被训练到。反而反之，因为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.282ex" height="1.339ex" role="img" focusable="false" viewBox="0 -442 1008.6 592"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container>地位和<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.282ex" height="1.339ex" role="img" focusable="false" viewBox="0 -442 1008.6 592"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container>地位相同，若<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.282ex" height="1.339ex" role="img" focusable="false" viewBox="0 -442 1008.6 592"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container>=0，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.282ex" height="1.339ex" role="img" focusable="false" viewBox="0 -442 1008.6 592"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container>不为0，则对<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.282ex" height="1.339ex" role="img" focusable="false" viewBox="0 -442 1008.6 592"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container>求导仍可训练。</li></ul><h5 id="LoRA的普适性"><a href="#LoRA的普适性" class="headerlink" title="LoRA的普适性"></a>LoRA的普适性</h5><p>LoRA作为一种参数高效微调 Parameter-Efficient Fine-Tuning, <strong>PEFT</strong>), 其核心是“低秩适应”这一数学思想，因此它的应用范围远不止图像生成领域。基于对上述原理的观察，在下认为，任何模型，只要其包含权重矩阵，都可以利用LoRA进行微调，甚至可以有选择地对部分权重矩阵进行微调。ps: 有文章明确提到了对Transformer使用LoRA<sup id="fnref:8_0"><a class="link" href="javascript:void(0);" rel="footnote" onclick="footnote_rememberSource('8', '8_0'); footnote_jumpToAnchor('fn:8')">[8]<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></sup>。</p><h4 id="使用LoRA"><a href="#使用LoRA" class="headerlink" title="使用LoRA"></a>使用LoRA</h4><p>即便LoRA大大降低了成本，由于模型的参数量很大，使用个人消费级显卡仍然可能很难训练。o(╥﹏╥)o</p><p>然天无绝人之路，我们还有<a class="link" href="https://www.modelscope.cn/aigc/modelTraining">魔搭社区的LoRA训练功能<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>可以免费使用！</p><p>而且阿里的工程师大佬说，他们是打算长期让这个功能免费的，本身便没有打算盈利，而是更多地希望推动技术的发展。(✧∇✧)</p><p>其实就算不是自行训练LoRA，使用社区里别的大佬训好的LoRA试玩一下也是很有意思的。下面这是我使用社区中<em>miratsu style</em> LoRA模型，参考相关提示词简单生成的图像 —— 无恶意  ^(*￣(oo)￣)^ </p><img lazyload="" src="/images/loading.svg" data-src="https://pub-a1cb67b2f5c34421bbd8c98bcf68643b.r2.dev/img/2025/09/03/可爱miku-20250902.jpeg" alt="你是这么大的猪" style="zoom: 33%;"><h2 id="推荐模型"><a href="#推荐模型" class="headerlink" title="推荐模型"></a>推荐模型</h2><p>图像生成工具</p><ul><li>魔搭 AIGC 图像生成专区：<a class="link" href="https://modelscope.cn/aigc/imageGeneration?tab=advanced">https://modelscope.cn/aigc/imageGeneration?tab=advanced<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li><li>开源项目 DiffSynth-Studio：<a class="link" href="https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/flux">https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/flux<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li></ul><p>视频生成工具</p><ul><li>魔搭 AIGC 视频生成专区：<a class="link" href="https://modelscope.cn/aigc/videoGeneration">https://modelscope.cn/aigc/videoGeneration<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li><li>开源项目 DiffSynth-Studio：<a class="link" href="https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/wanvideo">https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/wanvideo<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li></ul><p>音频生成工具</p><ul><li>CosyVoice：<a class="link" href="https://www.modelscope.cn/studios/iic/CosyVoice2-0.5B">https://www.modelscope.cn/studios/iic/CosyVoice2-0.5B<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li><li>ACE-Step：<a class="link" href="https://modelscope.cn/studios/ACE-Step/ACE-Step">https://modelscope.cn/studios/ACE-Step/ACE-Step<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li></ul><p>API服务</p><ul><li>百炼：<a class="link" href="https://bailian.console.aliyun.com/">https://bailian.console.aliyun.com/<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li></ul><p>免费云计算资源（A10，24G显存暂时无法支持大模型推理和训练）<br>魔搭 Notebook：<a class="link" href="https://modelscope.cn/my/mynotebook">https://modelscope.cn/my/mynotebook<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><img lazyload="" src="/images/loading.svg" data-src="https://pub-a1cb67b2f5c34421bbd8c98bcf68643b.r2.dev/img/2025/09/03/%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B-20250902.png" alt="推荐模型"></p><h3 id="推荐技术路线"><a href="#推荐技术路线" class="headerlink" title="推荐技术路线"></a>推荐技术路线</h3><p>针对于课程中提到的如何搓一个AI互动小游戏的推荐技术路线：</p><ul><li><p>训练图像生成模型的 LoRA（使用魔搭社区在线训练）</p></li><li><p>编写脚本、分镜描述（手动编写或使用 LLM 生成）</p></li><li><p>批量生成图像（使用魔搭 Notebook 调用 API）</p></li><li><p>将部分图像转为视频（使用魔搭在线生成，或使用其他工具）</p></li><li><p>生成配音（使用 CosyVoice 和 ACE-Step）</p></li><li><p>整合资源</p></li></ul><h1 id="前沿"><a href="#前沿" class="headerlink" title="前沿"></a>前沿</h1><h2 id="视频生成模型"><a href="#视频生成模型" class="headerlink" title="视频生成模型"></a>视频生成模型</h2><p>魔搭 AIGC 视频生成专区：<a class="link" href="https://modelscope.cn/aigc/videoGeneration">https://modelscope.cn/aigc/videoGeneration<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><h2 id="音频生成模型"><a href="#音频生成模型" class="headerlink" title="音频生成模型"></a>音频生成模型</h2><p>ACE-Step模型</p><p>CosyVoice</p><h2 id="多模态统一的模型架构"><a href="#多模态统一的模型架构" class="headerlink" title="多模态统一的模型架构"></a>多模态统一的模型架构</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://pub-a1cb67b2f5c34421bbd8c98bcf68643b.r2.dev/img/2025/09/03/%E5%A4%9A%E6%A8%A1%E6%80%81%E7%BB%9F%E4%B8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84-20250902.png" alt="多模态统一模型架构"></p><p>虽然这种能够同时处理多个模态的“万能模型”听起来很美好，但事实上现阶段它就像是……啥都能干，但啥都不精 —— <strong>博而不精</strong>。即模型虽然具备多种能力，但在任一单项任务上的表现，通常难以超越为该任务专门优化的顶尖模型。</p><p>但是学术界和工业界仍然在乐此不疲地研究，仍然值得期待</p><h2 id="展望"><a href="#展望" class="headerlink" title="展望"></a>展望</h2><h3 id="对齐训练与伦理道德"><a href="#对齐训练与伦理道德" class="headerlink" title="对齐训练与伦理道德"></a>对齐训练与伦理道德</h3><p><img lazyload="" src="/images/loading.svg" data-src="https://pub-a1cb67b2f5c34421bbd8c98bcf68643b.r2.dev/img/2025/09/03/%E4%BA%BA%E7%B1%BB%E6%95%B0%E6%8D%AEAI%E5%81%8F%E5%A5%BD-20250902.png" alt="人类数据AI偏好"></p><p>因为AI的能力是从数据集上学习得来的，而数据集又来自于人类，这意味着AI不可避免地会学习到一些属于人类的偏好甚至偏见，导致其输出内容可能带有某些特定色彩。</p><blockquote><p>AI始终是服务于人类的工具</p><p>其善恶之分</p><p>依赖于执掌者之心</p></blockquote><h2 id="额外课程"><a href="#额外课程" class="headerlink" title="额外课程"></a>额外课程</h2><h3 id="图像、音频、视频、html实践案例"><a href="#图像、音频、视频、html实践案例" class="headerlink" title="图像、音频、视频、html实践案例"></a>图像、音频、视频、html实践案例</h3><h3 id="Qwen-Image文生图、微调、编辑原理与实践"><a href="#Qwen-Image文生图、微调、编辑原理与实践" class="headerlink" title="Qwen-Image文生图、微调、编辑原理与实践"></a>Qwen-Image文生图、微调、编辑原理与实践</h3><section class="footnotes"><ol class="footnotes-list"><li id="fn:1">魔搭社区小编 . <a class="link" href="https://modelscope.cn/learn/1582?pid=1577">AIGC多模态生成：从实践到原理（附：从0手搓一个AI互动剧情小游戏）<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>. ModelScope, 2025<a class="link" href="javascript:void(0);" onclick="footnote_returnToSource('1')"> ↩<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li><li id="fn:2">ModelScope team. <a class="link" href="https://github.com/modelscope/DiffSynth-Studio">DiffSynth-Studio<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>. GitHub, 2025<a class="link" href="javascript:void(0);" onclick="footnote_returnToSource('2')"> ↩<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li><li id="fn:3">. [<a class="link" href="https://linux.do/t/topic/604467">探讨：中文提示词和英文提示词的效果区别<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>](https://linux.do/t/topic/604467). LINUX DO, 2025<a class="link" href="javascript:void(0);" onclick="footnote_returnToSource('3')"> ↩<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li><li id="fn:4">知白守黑. <a class="link" href="https://zhuanlan.zhihu.com/p/20494663092">Prompt资源精选|你想要的AI提示词都在这里了<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>. 知乎, 2025<a class="link" href="javascript:void(0);" onclick="footnote_returnToSource('4')"> ↩<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li><li id="fn:5">Rocky Ding. <a class="link" href="https://zhuanlan.zhihu.com/p/660924126">深入浅出完整解析ControlNet核心基础知识<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>. 知乎, 2025<a class="link" href="javascript:void(0);" onclick="footnote_returnToSource('5')"> ↩<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li><li id="fn:6">Hu, Edward J., et al. <a class="link" href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>. <em>arXiv preprint arXiv:2106.09685</em>, 2021<a class="link" href="javascript:void(0);" onclick="footnote_returnToSource('6')"> ↩<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li><li id="fn:7">大师兄. <a class="link" href="https://zhuanlan.zhihu.com/p/663557294">LoRA（Low-Rank Adaptation）详解<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>. 知乎, 2023<a class="link" href="javascript:void(0);" onclick="footnote_returnToSource('7')"> ↩<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li><li id="fn:8">Dreamweaver. <a class="link" href="https://zhuanlan.zhihu.com/p/636326003">大模型的领域适配 —— Parameter-Efficient Fine-Tuning (PEFT)<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>. 知乎，2023<a class="link" href="javascript:void(0);" onclick="footnote_returnToSource('8')"> ↩<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li></ol></section>]]></content>
    
    
    <summary type="html">阿里ModelScope AIGC课程《AIGC多模态生成：从实践到原理（附：从0手搓一个AI互动剧情小游戏）》笔记，融入了一些个人的理解和补充。踏进AIGC的魔法世界，成为一名AI caster吧！</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="AIGC" scheme="http://example.com/categories/AI/AIGC/"/>
    
    
    <category term="AIGC" scheme="http://example.com/tags/AIGC/"/>
    
    <category term="Course Note" scheme="http://example.com/tags/Course-Note/"/>
    
    <category term="AI" scheme="http://example.com/tags/AI/"/>
    
    <category term="practice" scheme="http://example.com/tags/practice/"/>
    
  </entry>
  
  <entry>
    <title>使用Git进行代码管理的经验谈</title>
    <link href="http://example.com/2025/08/30/Git-experience/"/>
    <id>http://example.com/2025/08/30/Git-experience/</id>
    <published>2025-08-30T14:08:03.000Z</published>
    <updated>2025-08-31T17:38:46.074Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章主要记录一些Git工具的使用方式，因为Git提供了很强大的功能，但是大部分功能在大多数情况下都不会用到，故在此简要记录方便未来作为速查表使用(使用ctrl+F可在浏览器快速查找关键词)。<br>除此之外也记录一些在基于Git的开源社区如GitHub上观察到的一些<strong>社区规范</strong>或者说暗语, 在进行项目代码管理或参与开源贡献的时候可以作为参考。<br>本文属于是不完全的梳理，所谓”经验谈”的意思其实就是当咱遇到了再进行相关的整理啦XD<br>会持续更新的orz<br>如有不恰当的地方或者想要补充的，欢迎在评论区交流 (^_−)☆</p><h2 id="使用Git"><a href="#使用Git" class="headerlink" title="使用Git"></a>使用Git</h2><h3 id="拉取和推送"><a href="#拉取和推送" class="headerlink" title="拉取和推送"></a>拉取和推送</h3><p>最常见的自然是<code>git pull</code>和<code>git fetch</code>，但是个人更喜欢先<code>fetch</code>, 再<code>merge</code>。因为事实上，<code>git pull</code>其实是一个组合件，它会先调用<code>git fetch</code>, 然后再根据情况选择调用<code>git rebase</code>或者<code>git merge</code><sup id="fnref:4_0"><a class="link" href="javascript:void(0);" rel="footnote" onclick="footnote_rememberSource('4', '4_0'); footnote_jumpToAnchor('fn:4')">[4]<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></sup><sup id="fnref:5_0"><a class="link" href="javascript:void(0);" rel="footnote" onclick="footnote_rememberSource('5', '5_0'); footnote_jumpToAnchor('fn:5')">[5]<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></sup> <del class="mask">(或许大多数时候，它调用的是merge，基本可以认为它会调用merge，但只要咱把它拆开来用，把一切的掌控权拿在自己手中，不就不用纠结这个问题了喵(￣▽￣)~*)</del></p><p>推荐流程:</p><ol><li><code>git fetch &lt;远程仓库别名&gt; &lt;分支名&gt;</code> 拉取远程仓库的指定分支的最新内容到本地，但<strong>不会直接合并</strong>, 先看看都发生了什么新的更改, 让自己拥有充分决策权</li><li><code>git merge &lt;远程仓库别名&gt;/&lt;分支名&gt;</code> 将您看中的分支合并到<strong>当前分支</strong>，若出现冲突，则需要解决冲突，并提交</li><li><code>git push &lt;远程仓库别名&gt; &lt;分支名&gt;</code> 将本地指定的分支上传到远端并<strong>和同名分支进行合并</strong></li></ol><ul><li>客观地说，最后一个部分可以改为<code>&lt;本地分支名&gt;:&lt;远程分支名&gt;</code>, 从而和指定分支进行合并，但是这真的有必要吗？中肯地说，<strong>在每次push之前，都应该先fetch远端所有必要的更改并在本地merge解决冲突之后，再进行push</strong>，所以最终分支名大抵是相同的。</li></ul><hr><h3 id="管理远程仓库"><a href="#管理远程仓库" class="headerlink" title="管理远程仓库"></a>管理远程仓库</h3><p><code>git remote</code>系命令可用于管理与当前仓库链接的远程仓库，具体用法如下：</p><div class="code-container" data-rel="Shell"><figure class="iseeu highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">git remote add &lt;别名&gt; &lt;仓库URL&gt;</span></span><br></pre></td></tr></table></figure></div><p>该命令可以添加一个远程仓库，其中’仓库URL’替换为目标仓库的URL，即xxx.git, ‘别名’替换为任意名称，如’origin’/‘upstream’<br>eg: <code>git remote add upstream https://github.com/ElysiaFollower/ElysiaFollower.github.io.git</code><br>在下认为这是这个分支<strong>最重要的一个命令</strong></p><p>至于添加远程仓库以后？那就基本可以完全抛却负担，像正常使用git一样进行使用啦。因为当我们平时git clone一个仓库的时候，实际上只是git clone自动帮我们添加了一个远程仓库, 本质是一样的<sup id="fnref:2_0"><a class="link" href="javascript:void(0);" rel="footnote" onclick="footnote_rememberSource('2', '2_0'); footnote_jumpToAnchor('fn:2')">[2]<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></sup>。<br>关于这个问题，其实下面这条命令正好可以进行一定程度的佐证:<br><code>git remote -v</code><br>可以查看当前仓库的远程仓库<br>举个例子:</p><div class="code-container" data-rel="Shell"><figure class="iseeu highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(base) PS D:\myblog\blog_in_develop\themes\redefine&gt; git remote -v</span><br><span class="line">origin  https://github.com/ElysiaFollower/hexo-theme-redefine.git (fetch)</span><br><span class="line">origin  https://github.com/ElysiaFollower/hexo-theme-redefine.git (push)</span><br><span class="line">upstream        https://github.com/EvanNotFound/hexo-theme-redefine.git (fetch)</span><br><span class="line">upstream        https://github.com/EvanNotFound/hexo-theme-redefine.git (push)</span><br></pre></td></tr></table></figure></div><p>这是在下fork的一个仓库，这里的origin仓库就是git clone时自动产生的，而upstream仓库则是在下手动添加的。可以看到，在<code>git remote -v</code>眼里，他俩是同一回事。<br>理解到此为止，所以添加之后只需要像正常一样使用<code>git push</code>和<code>git fetch</code>进行代码管理即可。<br>不过需要注意的是，git fetch命令的’完整版’其实应该是<sup id="fnref:2_1"><a class="link" href="javascript:void(0);" rel="footnote" onclick="footnote_rememberSource('2', '2_1'); footnote_jumpToAnchor('fn:2')">[2]<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></sup><sup id="fnref:3_0"><a class="link" href="javascript:void(0);" rel="footnote" onclick="footnote_rememberSource('3', '3_0'); footnote_jumpToAnchor('fn:3')">[3]<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></sup> :<br><code>git fetch &lt;远程仓库别名&gt; &lt;分支名&gt;</code><br>只是git clone的时候，同时生成了一些缺省时的默认配置，但是在管理多个远程仓库的时候还是<strong>全部显式声明更加所见即所得</strong>。</p><blockquote><p>When git fetch is run without specifying what branches and/or tags to fetch on the command line, e.g. git fetch origin or git fetch, remote.<repository>.fetch values are used as the refspecs—​they specify which refs to fetch and which local refs to update<sup id="fnref:3_1"><a class="link" href="javascript:void(0);" rel="footnote" onclick="footnote_rememberSource('3', '3_1'); footnote_jumpToAnchor('fn:3')">[3]<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></sup>.</repository></p></blockquote><p>fetch如此，push也是同理的:<br><code>git push &lt;remote别名&gt; &lt;branch&gt;</code></p><p>查看远程仓库相关信息:<br><code>git remote show &lt;remote别名&gt;</code></p><p>删除远程仓库：<br><code>git remote remove &lt;remote别名&gt;</code></p><h2 id="社区暗语"><a href="#社区暗语" class="headerlink" title="社区暗语"></a>社区暗语</h2><h3 id="从Fork到Pull-Request-PR"><a href="#从Fork到Pull-Request-PR" class="headerlink" title="从Fork到Pull Request(PR)"></a>从Fork到Pull Request(PR)</h3><p>不知道大家是否会经常听到PR这个词呀，在下第一次听到这个词的时候，还以为是peer review的缩写<del class="mask">(死去的fds和ads课程回忆突然开始攻击我)</del>, 但是代入上下文语境总觉得有些违和。后来了解之后才知道在开源共享或者项目管理中，<strong>PR</strong>往往指的实际上是<strong>Pull Request</strong>，即<strong>拉取请求</strong> —— <strong>您向原项目/仓库的维护者提交请求，建议他们合并你所做的修改</strong><br>在我的观察中，PR实际上可以理解为: 存在一个仓库A，而我们的手里有一个仓库B，这是基于A的延伸，或者基于A的修改，我们在仓库B的git历史中存在某一个分支，就像正常分支合并一样，我们可以<strong>通过Pull Request将这个分支合并/merge到仓库A的指定分支上中</strong>。 </p><h4 id="PR的好处都有啥"><a href="#PR的好处都有啥" class="headerlink" title="PR的好处都有啥"></a>PR的好处都有啥</h4><p>PR的意义尤其在于<strong>开源贡献</strong>。<br>请设想这样一个场景: 阁下在网上冲浪的时候发现了一份写得很好的代码，或者自己在使用某个开源工具的过程中体验到了一些难处，作者一直没有解决这个问题于是你决定自己动手。但是当您兴致满满地git clone完原仓库，幸苦完成了修改之后，自信满满地键入<code>git push origin xxx-new-feature</code>，您发现这个操作失败了，然后您意识到——您根本没有这个仓库的提交权限！若阁下尝试去直接联系仓库的维护者，且不说对方是否能够及时回复，就算回复了，对方可能也会出于<em>不想有人把代码搞得一团糟</em>甚至<em>删库跑路</em>等顾忌而不愿轻易提供写权限。<br>那么难道这就死局了吗？非也非也，聪明的程序员们发明了PR机制来解决这个问题。它有以下几个特点(根据在<em>GitHub</em>上的观察):</p><ol><li><strong>即便是对仓库没有写权限的人也可以发起PR</strong></li><li><strong>当阁下发起PR的时候，阁下可以为您的修改写一份声色俱茂的小作文，包括目的、动机、实现方法、效果和测试结果等</strong></li><li><strong>仓库的维护者可以直接查看您的修改<del class="mask">(diff形式)</del>，如果认为合适就可以合并到原仓库中，完成合并则等效于真的在原仓库中开了这么一条分支，在开发后合并到特定分支</strong></li><li><strong>PR拥有评论区，您可以和仓库的维护者或者其他人在这里展开充满激情的辩论。如果您认为他人说的有道理，您可以回到自己的分支中修改代码<del class="mask">(GitHub的PR会自动更新最新提交内容-2025/8/19)</del>，然后您就会惊奇发现，当时那条PR的commit记录自动延长了，阁下可以马上附上更新说明，在一个连续的历史上下文中完善您的这一次贡献</strong></li></ol><p>考虑到PR拥有的这些强大的能力——特别是“小作文”和评论区的设计——实际上为代码审查提供了很大的方便。所以在下肤浅的认为，对于一些大型项目的大型改动来说，即便是对于拥有写权限的开发者来说，学习使用PR也是非常有意义的！<br>虽然网上能搜到的一些协作开发的范式大都是<strong>不同人负责不同的分支</strong>，开发完了合并。或许这对于较为小型的项目来说更为方便，但是试想那位进行merge的人需要承担多大的心里负担呐。<strong>擅自</strong>将自己开发完的分支<strong>合并</strong>到别人的分支或者main分支上，说不定过段时间就会有一位悲惨的程序员发现自己原来能跑的代码跑不了了，于是悄悄地破防了(；´д｀)ゞ。如果联系项目负责人，让Ta进行<strong>审查合并</strong>，且不说联系过程怪麻烦的，而且整个审查并不是<strong>公开透明</strong>的，未来出现了问题也增加了维护的难度。<br>看，这些问题通过PR机制都能得到很好的解决ヽ(￣▽￣)ﾉ</p><h4 id="Fork实乃何物"><a href="#Fork实乃何物" class="headerlink" title="Fork实乃何物"></a>Fork实乃何物</h4><p>PR这么有用，那么怎么PR呢。这时候就不得不提Fork了。在下以为<strong>Fork+PR</strong>的组合拳，已经是进行PR的一种<strong>社区规范</strong>了。</p><p><strong>Fork</strong>实际上就是将一个仓库复制到自己的账号下，形成一个复制仓库，这样您就有了这个项目的副本，<strong>可以随意进行修改，而不会影响到主项目</strong>。</p><ul><li>也就是说，除了要发起PR的分支，阁下甚至可以开几条属于自己的<strong>草稿分支</strong>，并且不用担心遭到同伴亲切的问候“ <em>您在做神魔(O_o)??</em> ”</li></ul><p>至于怎么发起PR，则只需要在GitHub web端进入Fork形成的仓库副本，在Pull Request栏发起PR请求即可。事实上于在下的印象之中，当已经commit了几条之后，Fork的仓库首页就会贴心地提示您”Compare &amp; pull request”<sup id="fnref:1_0"><a class="link" href="javascript:void(0);" rel="footnote" onclick="footnote_rememberSource('1', '1_0'); footnote_jumpToAnchor('fn:1')">[1]<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></sup></p><h4 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h4><p>在Fork的仓库中，阁下可以通过<code>git remote add upstream &lt;原仓库URL&gt;</code>来添加一个<strong>上游仓库</strong><del class="mask">(<a href="#%E7%AE%A1%E7%90%86%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93">对应的git操作请参考</a>)</del>，这样您就可以通过<code>git fetch upstream</code>来将原仓库的最新代码拉取到自己的仓库中并通过<code>git merge upstream/branch_name</code>合并了！(￣▽￣)~*<br>这样的话，就算是团队内部进行开发或者修改自己使用中的工具，使用<strong>Fork仓库也完全无负担</strong>，反而更方便，自由度更高</p><ul><li>不仅可以无缝追踪原仓库进度</li><li>还有自己的草稿区</li></ul><p>注: 一般使用upstream作为原仓库的别名</p><hr><h3 id="Commit-Message规范"><a href="#Commit-Message规范" class="headerlink" title="Commit Message规范"></a>Commit Message规范</h3><p>网上很有多关于commits规范的说法，有的显得很复杂。在下以为，重要的还是<strong>能不能把事情说清楚</strong><del class="mask">(看到阿里在文章中说”建议使用中文（感觉中国人用中文描述问题能更清楚一些）”, 笑罢觉得还是有些事情值得反思的，形式主义没有意义)</del>，所以这里摘些个人最认可也认为是最重要的部分充当一下互联网的记忆叭(￣^￣) <sup id="fnref:6_0"><a class="link" href="javascript:void(0);" rel="footnote" onclick="footnote_rememberSource('6', '6_0'); footnote_jumpToAnchor('fn:6')">[6]<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></sup><sup id="fnref:7_0"><a class="link" href="javascript:void(0);" rel="footnote" onclick="footnote_rememberSource('7', '7_0'); footnote_jumpToAnchor('fn:7')">[7]<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></sup></p><p>最值得参考的大抵还是<strong>Angular规范</strong><sup id="fnref:8_0"><a class="link" href="javascript:void(0);" rel="footnote" onclick="footnote_rememberSource('8', '8_0'); footnote_jumpToAnchor('fn:8')">[8]<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></sup>和后来在它的基础上发展出来的<strong>Conventional Commits</strong>叭<sup id="fnref:6_1"><a class="link" href="javascript:void(0);" rel="footnote" onclick="footnote_rememberSource('6', '6_1'); footnote_jumpToAnchor('fn:6')">[6]<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></sup><br><strong>Angular范式</strong>：</p><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;</span><br><span class="line"></span><br><span class="line">&lt;body&gt;</span><br><span class="line"></span><br><span class="line">&lt;footer&gt;</span><br></pre></td></tr></table></figure></div><p>作用域scope，body和footer都是可选的；前面几个都挺有用的，相对而言footer的可有可无性略高</p><p>然后类型这边在下稍微糅合了一下，但都是接受度比较高的类型:</p><ul><li><strong>feat</strong>: 新功能</li><li><strong>fix</strong>: 修复bug</li><li>docs: 文档</li><li>style: 样式修改 —— 主要是代码样式修改，不影响运行的那种<sup id="fnref:6_2"><a class="link" href="javascript:void(0);" rel="footnote" onclick="footnote_rememberSource('6', '6_2'); footnote_jumpToAnchor('fn:6')">[6]<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></sup></li><li><strong>refactor</strong>: 重构，代码重构，逻辑重构</li><li><strong>test</strong>: 测试, 例如添加、删除、修改代码的测试用例等。</li><li>chore: 用于对非业务性代码进行修改，例如修改构建流程或者工具配置等</li><li>build: 用于修改项目构建系统，例如修改依赖库、外部接口或者升级 Node 版本等</li><li>ci: 用于修改持续集成流程，例如修改 Travis、Jenkins 等工作流配置；</li><li><strong>perf</strong>: 用于优化性能，例如提升代码的性能、减少内存占用等；</li><li>revert: 用于回滚版本</li></ul><p>footer里面的’<strong>BREAKING CHANGE</strong>‘是什么？—— 简单来说就是<strong>不兼容修改</strong>, 本次提交修改后将不兼容之前版本的API或者环境变量。建议写上<strong>Before</strong>和<strong>After</strong>，分别表示修改前的和修改后的版本。</p><p>关于怎么写，学理论不如看几个例子感受一下(。-`ω´-) ;取自<sup id="fnref:8_1"><a class="link" href="javascript:void(0);" rel="footnote" onclick="footnote_rememberSource('8', '8_1'); footnote_jumpToAnchor('fn:8')">[8]<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></sup> :<br><img lazyload="" src="/images/loading.svg" data-src="https://pub-a1cb67b2f5c34421bbd8c98bcf68643b.r2.dev/img/Git-experience/commit-example2.png" alt="example2"><br><img lazyload="" src="/images/loading.svg" data-src="https://pub-a1cb67b2f5c34421bbd8c98bcf68643b.r2.dev/img/Git-experience/commit-example1.png" alt="example1"></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><section class="footnotes"><ol class="footnotes-list"><li id="fn:1">Coding Is Fun. <a class="link" href="https://blog.csdn.net/u010295555/article/details/144091180">开源贡献：从 Fork 到 Pull Request（PR）<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>. CSDN, 2024<a class="link" href="javascript:void(0);" onclick="footnote_returnToSource('1')"> ↩<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li><li id="fn:2">Git. <a class="link" href="https://git-scm.com/book/zh/v2/Git-%e5%9f%ba%e7%a1%80-%e8%bf%9c%e7%a8%8b%e4%bb%93%e5%ba%93%e7%9a%84%e4%bd%bf%e7%94%a8">2.5 Git 基础 - 远程仓库的使用<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>. git-scm,<a class="link" href="javascript:void(0);" onclick="footnote_returnToSource('2')"> ↩<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li><li id="fn:3">Git. <a class="link" href="https://git-scm.com/docs/git-fetch">git-fetch - Download objects and refs from another repository<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>. git-scm, 2025<a class="link" href="javascript:void(0);" onclick="footnote_returnToSource('3')"> ↩<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li><li id="fn:4">Runner_Jack. <a class="link" href="https://www.cnblogs.com/runnerjack/p/9342362.html">git fetch &amp; pull详解<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>. 博客园, 2018<a class="link" href="javascript:void(0);" onclick="footnote_returnToSource('4')"> ↩<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li><li id="fn:5">Git. <a class="link" href="https://git-scm.com/docs/git-pull">git-pull - Fetch from and integrate with another repository or a local branch<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>. git-scm, 2025<a class="link" href="javascript:void(0);" onclick="footnote_returnToSource('5')"> ↩<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li><li id="fn:6">. <a class="link" href="https://www.conventionalcommits.org/zh-hans/v1.0.0/#%e7%ba%a6%e5%ae%9a%e5%bc%8f%e6%8f%90%e4%ba%a4%e8%a7%84%e8%8c%83">convertionalcommits<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>. ,<a class="link" href="javascript:void(0);" onclick="footnote_returnToSource('6')"> ↩<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li><li id="fn:7">阿里云开发者​. <a class="link" href="https://zhuanlan.zhihu.com/p/182553920">如何规范你的Git Commit?<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>. 知乎, 2025<a class="link" href="javascript:void(0);" onclick="footnote_returnToSource('7')"> ↩<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li><li id="fn:8">angular. <a class="link" href="https://github.com/angular/components/blob/main/CONTRIBUTING.md">AngularJS Git Commit Message Conventions<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a> <a class="link" href="https://docs.google.com/document/d/1QrDFcIiPjSLDn3EL15IJygNPiHORgU1_OOAqWjiDU5Y/preview?tab=t.0#heading=h.uyo6cb12dt6w">document<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>. github,<a class="link" href="javascript:void(0);" onclick="footnote_returnToSource('8')"> ↩<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li></ol></section>]]></content>
    
    
    <summary type="html">一些使用Git的方法，用作速查表; 同时记录一些在开源社区观察到的部分代码管理&quot;规范&quot;</summary>
    
    
    
    <category term="Tools" scheme="http://example.com/categories/Tools/"/>
    
    <category term="code" scheme="http://example.com/categories/Tools/code/"/>
    
    
    <category term="Git" scheme="http://example.com/tags/Git/"/>
    
    <category term="manual" scheme="http://example.com/tags/manual/"/>
    
    <category term="code management" scheme="http://example.com/tags/code-management/"/>
    
  </entry>
  
  <entry>
    <title>重读经典：Word2Vec如何通过“简化”撬动了整个NLP世界？</title>
    <link href="http://example.com/2025/08/25/@mikolov2013EfficientEstimationWord/"/>
    <id>http://example.com/2025/08/25/@mikolov2013EfficientEstimationWord/</id>
    <published>2025-08-25T13:45:00.000Z</published>
    <updated>2025-09-02T15:45:02.653Z</updated>
    
    <content type="html"><![CDATA[<p><strong>摘要</strong>: 2013年，一篇名为《Efficient Estimation of Word Representations in Vector Space》的论文横空出世，它提出的构成Word2Vec核心思想的CBOW和Skip-gram架构，彻底改变了我们让机器理解语言的方式。本文将带您重温这篇里程碑式的著作，探讨其核心思想：为何一个看似更“简单”的模型，反而能发现语言中令人惊叹的深层结构？</p><p>原文链接：<a class="link" href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><del class="mask">注：为了无障碍理解这篇论文，阁下可能需要先理解文末 <a href="#%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86">前置知识</a> 中提到的一些基本概念。</del></p><h2 id="问题的根源：昂贵的“理解”"><a href="#问题的根源：昂贵的“理解”" class="headerlink" title="问题的根源：昂贵的“理解”"></a>问题的根源：昂贵的“理解”</h2><p>在Word2Vec出现之前，自然语言处理（NLP）领域长期面临一个难题：如何让计算机理解词语之间的关系？传统方法如One-hot编码，将每个词视为一个独立的符号，这使得“国王”和“女王”在模型眼中毫无关联。</p><blockquote><p>as these(notion of words) are represented as indices in a vocabulary</p></blockquote><p>虽然当时先进的神经网络语言模型（NNLM）已经可以通过“分布式表示”（Distributed Representation）将词语学习为低维、稠密的向量（即<strong>词向量</strong>），并在一定程度上捕捉到语义相似性。但这些模型通常包含一个或多个复杂的<strong>非线性隐藏层</strong>。这个隐藏层是模型强大表达能力的核心，却也成为了<strong>巨大的计算瓶颈</strong>。其复杂度，特别是Hidden Layer的计算<del class="mask">（N×D×H part）</del>，使得在数十亿词级别的海量数据集上训练模型变得不切实际。简单来说，当时的“理解”非常昂贵，以至于我们无法用足够多的数据去“喂养”它。</p><blockquote><p>The main observation from the previous section was that most of the complexity is caused by the non-linear hidden layer in the model</p></blockquote><h2 id="解决方案：-大道至简的两个模型"><a href="#解决方案：-大道至简的两个模型" class="headerlink" title="解决方案： 大道至简的两个模型"></a>解决方案： 大道至简的两个模型</h2><p>面对上述瓶颈，Mikolov和他的同事们提出了一个颠覆性的思路：<br><strong>能不能用一个更简单的模型，来换取在更大规模数据上训练的能力？</strong><br>于是，他们大胆地<strong>移除了计算成本高昂的非线性隐藏层</strong>，从而极大地降低了计算复杂度。<br>答案就是本文提出的两个核心架构：CBOW 和 Skip-gram。基本可以认为，这两个模型就是后来广为人知的<strong>Word2Vec</strong>。</p><p><img lazyload="" src="/images/loading.svg" data-src="https://pub-a1cb67b2f5c34421bbd8c98bcf68643b.r2.dev/img/@mikolov2013EfficientEstimationWord/Model-architecture.png" alt="Model-architecture"></p><ol><li><strong>Continuous Bag-of-Words (CBOW)</strong></li></ol><ul><li>任务：根据上下文（周围的词）来预测中心词。</li><li>做法：将目标词周围N个词的向量从投影矩阵<del class="mask">(一个巨大的查询表，每一行即为一个词的向量)</del>中取出，然后直接对它们进行求和平均，形成一个汇总的上下文向量，并用这个向量去预测中心词 。因为忽略了上下文的词序，所以被称为“词袋”（Bag-of-Words）模型。</li><li>特点：多对一的预测，训练速度更快，对高频词效果更好。</li></ul><ol start="2"><li><strong>Continuous Skip-gram</strong></li></ol><ul><li>任务：与CBOW相反，它根据当前的中心词来预测其上下文。</li><li>做法：将中心词的向量作为输入，去预测其前后一定范围（窗口C）内的多个上下文词语。并且通过将窗口的大小设计为固定范围的随机数，便可实现统计意义上的对远端词语进行更少采样，从而给予近处上下文更高的权重，这或许可以被看作是一种朴素的、非动态学习的“注意力”思想的雏形。</li><li>特点：一对多的预测，训练时间更长，但能学习到更好的低频词表示，在大型语料上表现通常更优。论文的实验结果（如Table 3）也证实了这一点，Skip-gram在语义准确性上以55%对24%的巨大优势超过了CBOW。<br><del class="mask">和CBOW不同，Skip-gram的训练模式可能会有些难以理解，容易产生歧义，具体讲解可见 <a href="#Skip-gram%E8%AE%AD%E7%BB%83%E6%9C%BA%E7%90%86">Skip-gram训练机理</a></del></li></ul><p>这种简化带来的好处是惊人的。计算复杂度从NNLM的<code>Q = N×D + N×D×H + H×V</code> 急剧下降到CBOW的 <code>Q = N×D + N×log_2(V)</code> 和 Skip-gram的 <code>Q = Cx(D + D×log_2(V))</code>。正是这种效率上的巨大飞跃，使得在一天之内处理完16亿词的语料成为可能 ，也为接下来那个“魔法般”的发现奠定了基础。</p><p>注：这里提到的<code>Q</code>基本上可以理解为模型一次预测所涉及的参数量; </p><blockquote><p>Similar to [18], to compare different model architectures we define first the computational complexity of a model as the number of parameters that need to be accessed to fully train the model.</p></blockquote><p>注：此外，V到log_2(V)的变化是通过使用词汇的二叉树表示实现的，也就是说这对于NNLM的复杂度优化也是有用的，但是优化完之后，性能瓶颈在<code>NxDxH</code>, 仍然没有消失。</p><blockquote><p>With binary tree representations of the vocabulary, the number of output units that need to be evaluated can go down to around log2(V ). Thus, most of the complexity is caused by the term N × D × H.</p></blockquote><h2 id="惊人的发现：向量空间中的线性关系"><a href="#惊人的发现：向量空间中的线性关系" class="headerlink" title="惊人的发现：向量空间中的线性关系"></a>惊人的发现：向量空间中的线性关系</h2><p>如果说高效的模型是这篇论文的“肌肉”，那么它揭示的向量线性关系就是其“灵魂”。论文最令人振奋的发现是，通过上述简单模型在海量数据上训练出的词向量，不仅仅是让“猫”和“狗”在空间中彼此靠近，更是捕捉到了词语之间丰富、可量化的<strong>类比关系</strong>（Analogy）。</p><p>这种关系可以通过简单的向量代数运算来揭示。最经典的例子莫过于：<br><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="73.112ex" height="2.396ex" role="img" focusable="false" viewBox="0 -809 32315.7 1059"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mi" transform="translate(485,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(951,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(1384,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(1745,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(2230,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="msup" transform="translate(2681,0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mo" transform="translate(422,413) scale(0.707)"><g data-c="2033"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" transform="translate(275,0)"></path></g></g></g><g data-mml-node="mi" transform="translate(3541.9,0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="mi" transform="translate(4430.9,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(4775.9,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="msup" transform="translate(5375.9,0)"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(510,413) scale(0.707)"><g data-c="2033"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" transform="translate(275,0)"></path></g></g></g><g data-mml-node="mo" transform="translate(6324.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(6936,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(7936.3,0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mi" transform="translate(8421.3,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(8887.3,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(9320.3,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(9681.3,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(10166.3,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="msup" transform="translate(10617.3,0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mo" transform="translate(422,413) scale(0.707)"><g data-c="2033"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" transform="translate(275,0)"></path></g></g></g><g data-mml-node="mi" transform="translate(11478.2,0)"><path data-c="1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></g><g data-mml-node="mi" transform="translate(12529.2,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="msup" transform="translate(13058.2,0)"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(633,413) scale(0.707)"><g data-c="2033"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" transform="translate(275,0)"></path></g></g></g><g data-mml-node="mo" transform="translate(14130.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(14741.3,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(15741.5,0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mi" transform="translate(16226.5,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(16692.5,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(17125.5,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(17486.5,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(17971.5,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="msup" transform="translate(18422.5,0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mo" transform="translate(422,413) scale(0.707)"><g data-c="2033"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" transform="translate(275,0)"></path></g></g></g><g data-mml-node="mi" transform="translate(19283.4,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(20331.4,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(20816.4,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(21694.4,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="msup" transform="translate(22223.4,0)"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(633,413) scale(0.707)"><g data-c="2033"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" transform="translate(275,0)"></path></g></g></g><g data-mml-node="mo" transform="translate(23295.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(23962.1,0)"><path data-c="2248" d="M55 319Q55 360 72 393T114 444T163 472T205 482Q207 482 213 482T223 483Q262 483 296 468T393 413L443 381Q502 346 553 346Q609 346 649 375T694 454Q694 465 698 474T708 483Q722 483 722 452Q722 386 675 338T555 289Q514 289 468 310T388 357T308 404T224 426Q164 426 125 393T83 318Q81 289 69 289Q55 289 55 319ZM55 85Q55 126 72 159T114 210T163 238T205 248Q207 248 213 248T223 249Q262 249 296 234T393 179L443 147Q502 112 553 112Q609 112 649 141T694 220Q694 249 708 249T722 217Q722 153 675 104T555 55Q514 55 468 76T388 123T308 170T224 192Q164 192 125 159T83 84Q80 55 69 55Q55 55 55 85Z"></path></g><g data-mml-node="mi" transform="translate(25017.9,0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mi" transform="translate(25502.9,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(25968.9,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(26401.9,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(26762.9,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(27247.9,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="msup" transform="translate(27698.9,0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mo" transform="translate(422,413) scale(0.707)"><g data-c="2033"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" transform="translate(275,0)"></path></g></g></g><g data-mml-node="mi" transform="translate(28559.8,0)"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g><g data-mml-node="mi" transform="translate(29350.8,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(29922.8,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(30388.8,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="msup" transform="translate(30854.8,0)"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(633,413) scale(0.707)"><g data-c="2033"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" transform="translate(275,0)"></path></g></g></g><g data-mml-node="mo" transform="translate(31926.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container><br>这个等式石破天惊，<strong>它表明词向量空间中蕴含着抽象的语义维度</strong>，例如“性别”、“皇室”等。从“国王”的向量中减去“男人”的向量，相当于抽离出“男性”这个概念，保留了“皇室权威”等特征；再加上“女人”的向量，就将这个“皇室权威”的特征赋予了女性，最终得到的向量在空间中将接近“女王”对应的向量。<del class="mask">在这个空间中，词向量的距离可以表示语义的相似度。</del></p><p>为了系统性地验证这一发现，作者团队专门构建了一个包含<strong>语义(Semantic)和语法(Syntactic)类比问题</strong>的综合测试集<del class="mask">Semantic-Syntactic Word Relationship test set</del>。示例：<br><img lazyload="" src="/images/loading.svg" data-src="https://pub-a1cb67b2f5c34421bbd8c98bcf68643b.r2.dev/img/@mikolov2013EfficientEstimationWord/Semantic-Syntacic-test-set-example.png" alt="Semantic-Syntacic-test-set-example"></p><ul><li>语义类比: <em>Athens</em> is to <em>Greece</em> as <em>Oslo</em> is to? (<em>Norway</em>)</li><li>语法类比: <em>apparent</em> is to <em>apparently</em> as <em>rapid</em> is to? (<em>rapidly</em>)</li></ul><p>注: 在实操上，可以通过上述向量运算得到结果向量，最后在整个词汇表的向量中，寻找与这个结果向量距离最近的词向量，这个词就是模型的答案——论文中使用的距离度量为“余弦距离”。<br>在下以为，若所有词向量都预先进行归一化（使其长度为1），那么计算成本高昂的余弦距离就可以被简单的“向量点积”所替代。在单位球面上，按点积大小排序等同于按欧氏距离排序，但计算效率更高。ps: 单位球面上，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="22.454ex" height="3.048ex" role="img" focusable="false" viewBox="0 -1097 9924.7 1347"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2016" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(500,0)"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mo" transform="translate(264.5,31) translate(-250 0)"><path data-c="20D7" d="M377 694Q377 702 382 708T397 714Q404 714 409 709Q414 705 419 690Q429 653 460 633Q471 626 471 615Q471 606 468 603T454 594Q411 572 379 531Q377 529 374 525T369 519T364 517T357 516Q350 516 344 521T337 536Q337 555 384 595H213L42 596Q29 605 29 615Q29 622 42 635H401Q377 673 377 694Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1251.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2251.4,0)"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mo" transform="translate(214.5,283) translate(-250 0)"><path data-c="20D7" d="M377 694Q377 702 382 708T397 714Q404 714 409 709Q414 705 419 690Q429 653 460 633Q471 626 471 615Q471 606 468 603T454 594Q411 572 379 531Q377 529 374 525T369 519T364 517T357 516Q350 516 344 521T337 536Q337 555 384 595H213L42 596Q29 605 29 615Q29 622 42 635H401Q377 673 377 694Z"></path></g></g></g><g data-mml-node="msup" transform="translate(2680.4,0)"><g data-mml-node="mo"><path data-c="2016" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z"></path></g><g data-mml-node="mn" transform="translate(533,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(3894.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(4950.6,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mo" transform="translate(5672.8,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(6673,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(7339.7,0)"><path data-c="63" d="M370 305T349 305T313 320T297 358Q297 381 312 396Q317 401 317 402T307 404Q281 408 258 408Q209 408 178 376Q131 329 131 219Q131 137 162 90Q203 29 272 29Q313 29 338 55T374 117Q376 125 379 127T395 129H409Q415 123 415 120Q415 116 411 104T395 71T366 33T318 2T249 -11Q163 -11 99 53T34 214Q34 318 99 383T250 448T370 421T404 357Q404 334 387 320Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(444,0)"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(944,0)"></path></g><g data-mml-node="mo" transform="translate(8677.7,0)"><path data-c="2061" d=""></path></g><g data-mml-node="mo" transform="translate(8677.7,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(9066.7,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(9535.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></p><p>实验结果（论文中的Table 3, 4; 6）雄辩地证明，CBOW和尤其是Skip-gram模型，在这类任务上的准确率远超当时更复杂的NNLM和RNNLM模型，同时训练成本大幅降低 。<br><strong>准确率：</strong><br><img lazyload="" src="/images/loading.svg" data-src="https://pub-a1cb67b2f5c34421bbd8c98bcf68643b.r2.dev/img/@mikolov2013EfficientEstimationWord/Table3.png" alt="Table3"><br><img lazyload="" src="/images/loading.svg" data-src="https://pub-a1cb67b2f5c34421bbd8c98bcf68643b.r2.dev/img/@mikolov2013EfficientEstimationWord/Table4.png" alt="Table4"><br><strong>训练成本：</strong><br><img lazyload="" src="/images/loading.svg" data-src="https://pub-a1cb67b2f5c34421bbd8c98bcf68643b.r2.dev/img/@mikolov2013EfficientEstimationWord/Table6.png" alt="Table6"></p><h2 id="历史回响：前沿性"><a href="#历史回响：前沿性" class="headerlink" title="历史回响：前沿性"></a>历史回响：前沿性</h2><h3 id="它被推翻了吗？"><a href="#它被推翻了吗？" class="headerlink" title="它被推翻了吗？"></a>它被推翻了吗？</h3><ul><li>思想从未被推翻，反而成为了基石。 “<strong>将词语映射到稠密向量空间中，通过无监督学习捕捉其语义</strong>”这一核心思想，已经成为整个现代NLP的奠基性观念之一。</li></ul><h3 id="它还是最前沿吗？"><a href="#它还是最前沿吗？" class="headerlink" title="它还是最前沿吗？"></a>它还是最前沿吗？</h3><p>具体技术已演进，但思想永存。</p><ul><li><p>Word2Vec的局限在于它生成的是静态词向量<del class="mask">(训练得到一个固定的词向量查询表)</del>，对于一个token只有一个表示，<strong>无法处理一词多义问题</strong>（例如”bank”的“银行”和“河岸”两个含义共享同一个向量）。而当代的SOTA（State-of-the-Art）技术，如BERT、GPT等基于Transformer的模型，生成的是<strong>动态的、上下文相关的词向量</strong>。它们能根据句子语境为同一个词生成不同的表示，极大地提升了语义捕捉的精度。<del class="mask">大体上应该可以理解成：输入句子之后，句子中的所有词首先经过一层类似于Word2Vec的初始嵌入，然后在经过注意力机制处理后，再输出每个词最终的词向量</del></p></li><li><p>同时，CBOW由于其BOW的模型，缺乏对位置的建模； Skip-gram的位置概念也非常模糊，并未显式建模。</p></li><li><p>此外，word2vec模型本身并未提供直接获取整个句子向量的有效方法，通常采用的“词向量平均”策略会丢失重要的语序和语法信息。</p></li><li><p>然而，即便在今天，Word2Vec因其高效、轻量的特性，仍在学术研究和工业界中扮演着重要角色，甚至就连它被诟病的无法应对一词多义的问题有时也是一种优势——因为它可以得到一个稳定的向量训练结果，若要知晓某个词的向量，在向量表<del class="mask">(投影矩阵)</del>中直接查询即得。</p></li></ul><h2 id="杂谈"><a href="#杂谈" class="headerlink" title="杂谈"></a>杂谈</h2><p>《Efficient Estimation of Word Representations in Vector Space》是一篇充满“反直觉”智慧的论文。它告诉我们，<strong>有时最优雅的解决方案并非来自更复杂的模型</strong>，而是来自对问题本质的深刻洞察。通过极致的简化，Word2Vec将计算力从复杂的模型结构中解放出来，投入到对海量数据的学习中，最终发现了语言本身蕴含的、令人着迷的数学之美。这不仅是一次技术的胜利，更是一次思想的胜利。</p><h3 id="一些可能容易迷惑的点："><a href="#一些可能容易迷惑的点：" class="headerlink" title="一些可能容易迷惑的点："></a>一些可能容易迷惑的点：</h3><h4 id="最终产物"><a href="#最终产物" class="headerlink" title="最终产物"></a>最终产物</h4><p>虽然Word2Vec在训练时的确利用了上下文，但请注意，训练的最终产物是一个固定的查询表（即投影矩阵 <code>W</code>）。训练结束后，无论“bank”出现在什么新的句子中，我们去使用它的词向量时，做的动作仅仅是<strong>查表</strong>——从矩阵<code>W</code>中把“bank”对应的那一行向量取出来。这个向量是<strong>一次性生成、全局固定</strong>的，所以它是“静态”的、“上下文无关”的。上下文信息在训练时被“蒸馏”进了这个固定的向量里，但在使用时，新的上下文不起作用。</p><p>实际上这种方式训练出来的是一个既能进行嵌入又能进行预测的模型——正是因为有预测的能力，所以才能计算损失进行反向传播，从而不断调试投影矩阵，进而得到较好的词向量表示，也就得到了一个较好的嵌入模型。这或许也算是这类自监督学习（Self-supervised Learning）模型的训练哲学吧。我们并不真正关心模型预测得有多准，这个“预测任务”只是一个“借口”。我们真正的目标是，在完成这个任务的过程中，<strong>强迫模型学习到有意义的中间表示</strong>——也就是那个高质量的“投影矩阵”（词向量）</p><h4 id="共享投影层的说法"><a href="#共享投影层的说法" class="headerlink" title="共享投影层的说法"></a>共享投影层的说法</h4><p>论文在介绍模型架构时提到的<strong>共享投影层</strong>(using a shared projection matrix)可能会让人感到困惑。在下认为，这里的<strong>共享</strong>主要指的是对于上下文窗口的不同位置，使用是同一个投影矩阵进行映射；对比的是上下文窗口的每个位置分别维护不同的投影矩阵。除此之外，在<strong>CBOW</strong>中，作者还做了一个更激进的共享——投影后所处的空间位置也进行共享：输入的N个词向量在查找到之后，将被直接平均（Average）成一个单独的D维向量，空间位置信息被彻底抹除；对比在<strong>NNLM</strong>中，查到的向量会被拼接成<code>N*D</code>维的长向量</p><blockquote><p>Note that the weight matrix between the input and the projection layer <strong>is shared for all word positions in the same way</strong> as in the NNLM.</p></blockquote><h4 id="Skip-gram训练机理"><a href="#Skip-gram训练机理" class="headerlink" title="Skip-gram训练机理"></a>Skip-gram训练机理</h4><p>要理解Skip-gram，首先需要明确其核心精髓——它<strong>不追求单个预测的精准度</strong> <del class="mask">预测任务只是一个“借口”，我们真正的目标是在这个过程中“锤炼”出优秀的词向量</del></p><p>CBOW是将多个上下文单词加起来，然后用聚合后的向量一次性去预测中心词，每次训练就聚焦一次：目标词和使用聚合向量预测目标词的概率<br>而Skip-gram, 则从中心词出发，和上下文窗口<code>C</code>中的所有词分别组成一个独立的训练样本对，每次训练就聚焦<code>C</code>次：这一次的目标词和使用中心词向量预测该目标词的概率<br>例如：假设我们的句子是<code>I am a fool</code>, 窗口大小<code>C=1</code>, 当前处理的中心词是<code>a</code><br>则形成两个<strong>独立的</strong>训练样本对：</p><ol><li><code>a</code> -&gt; <code>am</code></li><li><code>a</code> -&gt; <code>fool</code><br>所以模型会训练两次，一次输入<code>a</code>，使用预测出<code>am</code>的概率计算损失；一次输入<code>a</code>，使用预测出<code>fool</code>的概率计算损失<del class="mask">在使用层级Softmax的时候损失计算可能没有这么简单，但本质不变</del><br>在反向传播的时候，不仅会调整输出层的权重，更重要的是也会<strong>调整<code>a</code>的词向量</strong>，从而不断拉扯<code>a</code>的词向量，让它更符合上下文语境</li></ol><h4 id="计算复杂度"><a href="#计算复杂度" class="headerlink" title="计算复杂度"></a>计算复杂度</h4><p>论文中给出的计算复杂度公式乍一看可能有些吓人。但根据论文定义，它估算的主要是训练时需要访问的参数数量，理解起来并不复杂。下面我们进行简要拆解：</p><ol><li><strong>NNLM</strong>: <code>Q = N×D + N×D×H + H×V</code></li></ol><ul><li><code>N×D</code>：这部分代表从输入层到投影层的计算。将N个上下文单词（通常是one-hot）通过投影矩阵映射为N个D维向量，再拼接成一个N×D维的向量。因为涉及到了投影矩阵中的N行，所以对应参数量为<code>N×D</code></li><li><code>N×D×H</code>：这是计算瓶颈，代表从投影层（N×D维）到非线性隐藏层（H维）的全连接计算。<del class="mask">在这篇论文的语境下，应该是默认其所对比的NNLM为仅拥有<strong>单层</strong>非线性隐藏层的经典模型</del></li><li><code>H×V</code>：代表从隐藏层（H维）到巨大无比的输出层（V维，V是词汇表大小）的全连接计算。</li></ul><ol start="2"><li><strong>CBOW</strong>: <code>Q = N×D + D×log₂(V)</code></li></ol><ul><li><code>N×D</code>：与NNLM类似，查询N个上下文单词的D维向量。</li><li><code>D×log₂(V)</code>：模型移除了N×D×H的隐藏层。将N个词向量平均后得到仅仅一个D维向量，直接用它来预测输出。同时，输出层使用<strong>层序Softmax</strong>（Hierarchical Softmax）这种技巧，将原本D×V的计算量降维打击至D×log₂(V)，因为它只需要在一个二叉树（深度约log₂(V)）上做一系列二分类判断，而不是在全部V个词上计算概率。所以这个技术对于NNLM同样适用，理论上上面NNLM的复杂度也可以优化，但是不影响CBOW和Skip-gram仍然远比它快。</li></ul><ol start="3"><li><strong>Skip-gram</strong>: <code>Q = C×(D + D×log₂(V))</code></li></ol><ul><li><code>D</code>：输入只有一个词，查询其D维向量。</li><li><code>D×log₂(V)</code>：用这个D维输入向量，去预测一个上下文单词。与CBOW一样，也用了层序Softmax优化。</li><li><code>C×(...)</code>：因为Skip-gram模型需要对一个输入词，预测其周围的C个上下文单词，所以上述过程需要重复C次。</li></ul><p><strong>一言以蔽之：Word2Vec的效率革命，主要来自（1）砍掉N×D×H的隐藏层 和（2）用log₂(V)复杂度的层序Softmax替换V复杂度的普通Softmax。</strong></p><p>关于层序Softmax的使用，感兴趣的话可以作者参考引用的这几篇论文</p><ul><li>T. Mikolov, A. Deoras, D. Povey, L. Burget, J. Cˇ ernocky ́. Strategies for Training Large Scale Neural Network Language Models, In: Proc. Automatic Speech Recognition and Understanding, 2011.</li><li>A. Mnih, G. Hinton. A Scalable Hierarchical Distributed Language Model. Advances in Neural Information Processing Systems 21, MIT Press, 2009.</li><li>F. Morin, Y. Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS, 2005.</li></ul><h2 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h2><ul><li><p><strong>词的表示方法：从One-hot到分布式表示</strong></p><ul><li><strong>One-hot</strong>：想象一个词典有10万个词，那么“apple”这个词可能被表示成一个10万维的向量，其中只有对应“apple”的位置是1，其余全是0。这种方式无法体现词与词之间的关系 。</li><li><strong>分布式表示 (Distributed Representation)</strong>：与One-hot不同，它用一个稠密的、低维度的向量（例如300维）来表示一个词。向量中的每一个维度都代表了词语的某种潜在特征。<strong>这篇论文的核心就是如何高效地学习这种表示</strong>。</li></ul></li><li><p><strong>全连接层</strong></p><ul><li>一个全连接层包含一个权重矩阵<code>W</code>和一个偏置向量<code>b</code>。当输入一个向量<code>x</code>时，输出为<code>y = Wx + b</code>。</li></ul></li><li><p><strong>神经网络语言模型 (NNLM) 的基本结构</strong></p><ul><li>阁下只需要大致了解它通常包含输入层、投影层、隐藏层和输出层。</li><li>其中<strong>投影层</strong>的概念可能会让人有点陌生，但对于理解本文思路又至关重要，所以这里在下简要介绍基本原理<ul><li>您可以把它想象成一个巨大的查询表。输入层的一个词就像一个开关，正好选中表中的某一行。这一行，就是一个低维、稠密的向量，它就是那个词的分布式表示。因此，这个投影层矩阵本身，在训练结束后，就成为了我们最终想要的“词向量词典”。</li></ul></li></ul></li><li><p><strong>向量空间与相似度</strong></p><ul><li>当词语被表示为向量后，我们就可以在多维空间中计算它们之间的距离（如余弦距离）。距离越近，代表这两个词语的语义或用法越相似 。</li></ul></li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] T. Mikolov, K. Chen, G. Corrado, and J. Dean. <a class="link" href="https://arxiv.org/abs/1301.3781">Efficient estimation of word representations in vector space<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>. <em>arXiv preprint arXiv:1301.3781</em>, 2013.</p>]]></content>
    
    
    <summary type="html">2013年，一篇名为《Efficient Estimation of Word Representations in Vector Space》的论文横空出世，它提出的构成Word2Vec核心思想的CBOW和Skip-gram架构，彻底改变了我们让机器理解语言的方式。本文将带您重温这篇里程碑式的著作，探讨其核心思想：为何一个看似更“简单”的模型，反而能发现语言中令人惊叹的深层结构？</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="basis" scheme="http://example.com/categories/AI/basis/"/>
    
    
    <category term="AI" scheme="http://example.com/tags/AI/"/>
    
    <category term="basis" scheme="http://example.com/tags/basis/"/>
    
    <category term="Word2Vec" scheme="http://example.com/tags/Word2Vec/"/>
    
    <category term="paper" scheme="http://example.com/tags/paper/"/>
    
    <category term="impressive" scheme="http://example.com/tags/impressive/"/>
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>公告(说明)</title>
    <link href="http://example.com/2025/05/09/announcement/"/>
    <id>http://example.com/2025/05/09/announcement/</id>
    <published>2025-05-09T08:47:48.000Z</published>
    <updated>2025-08-25T14:06:20.653Z</updated>
    
    <content type="html"><![CDATA[<p>本站仍处于建设过程中~<br>内容的补充和校对会在假期进行~🥲</p><blockquote><p>说是要在假期进行，但转眼间假期都要结束了…<br>正好近期没什么事，外加正好在做文献整理，便稍微建设一下叭(((</p></blockquote><p>如果有任何建议或意见，欢迎联系讨论~🥰</p><p>小站引用了许多来自网络的图片，能力有限，无法全部找出原作者orz😭<br>如需注明作者请联系博主,侵删🫶    </p>]]></content>
    
    
    <summary type="html">站点公告</summary>
    
    
    
    <category term="announcement" scheme="http://example.com/categories/announcement/"/>
    
    
    <category term="announcement" scheme="http://example.com/tags/announcement/"/>
    
  </entry>
  
</feed>
