[{"title":"MCP - image test","url":"/2025/05/08/MCP/","content":"introduction由claude的母公司Anthropic于2024年11月25日推出，全称模型上下文协议(Model Context Protocol)\n\n这是一个开放标准, 在该网站可以看到官方文档 https://modelcontextprotocol.io/\n\n中文文档：https://mcp-docs.cn/introduction\n\n\n旨在将人工智能应用与不同工具和数据源进行连接\n\n提供一种标准化的路径来沟通 模型 与 工具\n属于中间协议层， 就像USB-C转接器一样，统一外部server接入host， 如下图所示， 通过MCP client作为转接器，不同的Server最终以统一的接口形式暴露给host，让模型得知。\n\n\n\n\nMCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.\n\n\n\n本质上是在模型和外部工具间添加一个 中介程序， 对接口进行封装\n\n原先的外部工具套上一层壳(规范接口)就成了一个个server\n中介程序中由client 和 server 打交道\n通信基于MCP协议\n\n\n而容纳client的中介程序，就是MCP Host —— 例如vscode的cline插件\n\n具体架构见图 架构\n现实意义能够帮助我们在抽象的顶层设计设计agent或Workflow —— 相当于将模型与工具封装隔离，设计者不需要过多考虑交互的细节与融合。而现在的LLMs经常需要同外部数据或者工具进行融合，以此来增强自己的能力\n举个例子，假如没有MCP，那么不同如 Claude Desktop、IDE 或 AI 工具等AI应用，想要增加外部工具的调用，则需要彼此按照自己的村规对每个不同的工具做适配(比如function calling)\n而引入MCP协议作为中间层之后，一切变得井然有序, 如右图(目前的MCPserver数已经上万，如果没有中间层，这是不可想象的)\n在此之后，甚至可能可以基于MCP协议对模型进行专门的训练，使之对工具使用的接口更加熟悉，调用更加稳定。(因为本质上要求模型按规定格式输出)\n\n一般架构\nMCP Hosts: 如 Claude Desktop、IDE 或 AI 工具，希望通过 MCP 访问数据的程序\n\nMCP Clients: 维护与服务器一对一连接的协议客户端\n\nMCP Servers: 轻量级程序，通过标准的 Model Context Protocol 提供特定能力\n\n本地数据源: MCP 服务器可安全访问的计算机文件、数据库和服务\n\n远程服务: MCP 服务器可连接的互联网上的外部系统（如通过 APIs）\n\nMCP Hosts: Programs like Claude Desktop, IDEs, or AI tools that want to access data through MCP\n\nMCP Clients: Protocol clients that maintain 1:1 connections with servers\n\nMCP Servers: Lightweight programs that each expose specific capabilities through the standardized Model Context Protocol\n\nLocal Data Sources: Your computer’s files, databases, and services that MCP servers can securely access\n\nRemote Services: External systems available over the internet (e.g., through APIs) that MCP servers can connect to\n\n\n\n其中MCPserver虽然名字里带个server但它实际上完全可以是本地的一个程序，只是在逻辑关系上就像提供服务的server一样——因为本质上是外部工具的封装\n外部工具封装成MCP Server， 根据MCP协议，在传输层与MCP Client进行交互， client再将信息整合与Host交互\n\n传输层实现细节文档：https://modelcontextprotocol.io/docs/concepts/transports#standard-input%2Foutput-stdio\n有两种传输方式\n\nstdio(适用于本地进程，两者都在同一台计算机上)\n在这种模式下，server进程被client进程创建出来作为子进程，然后通过标准输入输出进行交互\nclient把消息写到server的stdin， server把response写到它的stdout\n\n\n\n\n基于HTTP的SSE传输(SSE: Server-Sent Events) \n使用 Server-Sent Events技术实现client到server的通信\nclient使用HTTP GET和HTTP POST与server进行交互\n\n\n通信的协议内容基于JSON-RPC2.0\n\n\n\n\n其实也允许自定义传输方式，但那样就偏离了统一接口的目的，因为又引入了未定义的行为\n\n核心概念\nResources\n\n它允许server暴露可以被 clients 读取并用作 LLM 交互上下文的数据和内容\n\nserver通过resources/list endpoint暴露可用资源，resource结构\n&#123;  uri: string;           // Unique identifier for the resource  name: string;          // Human-readable name  description?: string;  // Optional description  mimeType?: string;     // Optional MIME type&#125;\n\n或者\n&#123;  uriTemplate: string;   // URI template following RFC 6570  name: string;          // Human-readable name for this type  description?: string;  // Optional description  mimeType?: string;     // Optional MIME type for all matching resources&#125;\n\nclient可以通过resources/read 请求读取resources\n\n\n\n\n\nPrompts\n\n允许 servers 定义可复用的提示模板和工作流，可以被Client获取\n\nprompt结构\n&#123;  name: string;              // Unique identifier for the prompt  description?: string;      // Human-readable description  arguments?: [              // Optional list of arguments    &#123;      name: string;          // Argument identifier      description?: string;  // Argument description      required?: boolean;    // Whether argument is required    &#125;  ]&#125;\n\nclient可以通过prompts/list发现可用的prompts模版，然后嵌入必要的参数，通过prompts/get 进行使用\n\n\n\n\n\nTools\n\n使 servers 能够向 clients 暴露可执行功能\n\nTool结构\n&#123;  name: string;          // Unique identifier for the tool  description?: string;  // Human-readable description  inputSchema: &#123;         // JSON Schema for the tool&#x27;s parameters    type: &quot;object&quot;,    properties: &#123; ... &#125;  // Tool-specific parameters  &#125;,  annotations?: &#123;        // Optional hints about tool behavior    title?: string;      // Human-readable title for the tool    readOnlyHint?: boolean;    // If true, the tool does not modify its environment    destructiveHint?: boolean; // If true, the tool may perform destructive updates    idempotentHint?: boolean;  // If true, repeated calls with same args have no additional effect    openWorldHint?: boolean;   // If true, tool interacts with external entities  &#125;&#125;\n\nClients 可以通过 tools/list endpoint 列出可用的 tools\n\nClient 可以使用 tools/call endpoint 调用tool，servers 执行请求的操作并返回结果\n\n\n\n\n\nSampling\n\n它允许 servers 通过 client 请求 LLM 补全，从而实现复杂的 agentic 行为，同时保持安全性和隐私性\n\n这里产生了对LLM的请求，需要借用Host完成，而后文会提到这并不在MCP明确规定的范围内，所以未必所有的Host都支持该功能\n\n如Claude Desktop\n\nThis feature of MCP is not yet supported in the Claude Desktop client.\n\n\n\n\n\n\nRoots\n\n它定义了 servers 可以操作的边界\n告知 servers 相关 resources 及其位置信息\n明确哪些 resources 是你的 workspace 的一部分\n\n\n\n\n\n注意上述的这些核心概念，一般只有Tool部分是都会支持的，其它的具体要看client和host的实现\n\n工作过程Client 和 Server之间的通信\n\nClinet向Server发生初始化请求\nServer会告诉Client自己拥有哪些工具以及协议版本(&gt; protocol version and capabilities)\nClient向Server发送信息，确认初始化成功\n开始正常信息交流\nRequest-Response: Client或Server发生请求，另一方响应\nNotifications: 一方单方面发送消息，不要求响应\n\n\n\n当出现以下三种情况时终止\n\n使用close()干净关闭\n传输断开\n出现错误\n\nClient 和 模型之间的通信\nMCP协议主要规定的是client和server之间的通信，Client和模型之间的通信会受到host具体实现的影响，但这一部分仍然有助于我们理解一个MCP应用的工作流程\nAnthropic的官网上似乎并没有规定Client和AI模型之间的通信 —— 只是规定了client和server之间的通信\n\n\n\n以cline为例，有人通过数据抓包发现，在实际工作的时候，Cline插件并没有使用function calling技术，而是——直接把所有外部工具的详细信息直接写在了提示词中(json格式，系统提示词，”role”:”system”)，甚至还有Cline内置的工具，甚至还有详细的使用案例教学，长达上万字符。其中用户的信息也同样在这段提示词中，通过另一项json来进行指定(用户提示词，”role”: “user”)。\n所以发生的事情是这样的：\n\nHost(如Cline)， 将用户的require附加上关于MCP工具的所有使用说明提供给模型\n模型按照提示词指定的格式调用工具，如使用标签进行标记\n如果模型请求了工具调用，那么工具执行结果很可能会和原先的系统提示词、用户提示词一起发给模型，让它进行重新生成，直到没有工具调用请求。\n\n\n\n\n引自BV1sWoMYUEi9， https://zhuanlan.zhihu.com/p/29001189476\n\n\n工程实践以下是一些MCP server汇总网站\n\nhttps://smithery.ai/\nhttps://mcp.so/\nhttps://www.mcp-home.com/\nhttps://mcpmarket.com/\nhttps://mcpmarket.cn/\nhttps://mcpservers.org/\n\n使用示例找到一个心仪的MCP server(或者自己做一个)\n\n这里使用的MCP Server：https://mcp.so/server/fetch/modelcontextprotocol\n\n一般来说，他会是类似的界面，有一些说明，但最重要的是右侧红框标记的Server Config， 将其复制到MCP Host的MCP Server配置文件即可\n\n以cline作为客户端为例：\n找到一个配置选项，点击就会打开一个json文件\n\n在这个json文件复制进入从server提供者那里得到的server config即可——如果有多个的话，用逗号隔开。\n&#123;  &quot;mcpServers&quot;: &#123;    &quot;fetch&quot;: &#123;      &quot;command&quot;: &quot;uvx&quot;,      &quot;args&quot;: [&quot;mcp-server-fetch&quot;]    &#125;  &#125;&#125;\n以类似上述json格式去定义MCPserver其中fetch为一个MCPserver的名字，运行的程序为uvx，参数为mcp-server-fetch\n\n所以可知，需要首先下载uvx程序\nMCPserver大多使用python或者node进行编写，对应启动程序一般是(在MCP server供应处复制的时候看看command是什么就知道了)\npython - uvx(uv tool run的别名)\nnode - npx\n\n\n关于uv：\n\nAn extremely fast Python package and project manager, written in Rust.\n\n\npython包管理器\n\n\n文档：https://docs.astral.sh/uv/\nmac+linux安装： curl -LsSf https://astral.sh/uv/install.sh | sh\nwindows安装： powershell -ExecutionPolicy ByPass -c &quot;irm https://astral.sh/uv/install.ps1 | iex&quot;\n\n\n关于npx, 其为node的一部分，所以已经安装过Node.js的不需要做额外安装\nuvx&#x2F;npx之外的command同理(原理是通的，都是运行命令做一些事情)\n\n\n\n在客户端配置MCPserver的时候很多时候会运行命令、进行注册，所以要保证安装成功才能正常注册。\n\n注：在客户端配置完MCPserver可能会出现以下情况\n尝试运行命令-&gt;发现对应工具没有安装-&gt;开始下载安装-&gt;安装太久，超时-&gt;配置失败\n则解决方法为，现在自己的终端运行一遍，保证下载完成，例如uvx mcp-server-fetch\n下载完成可能需要手动退出，因为对应程序可能会被运行，然后开始等待输入\n\n\n\n\n\n\n\n在网站找到提供的MCPserver时，提供方提供的json可能是上面那个示例的格式但在实际配置的时候，可能会出现多两个参数disabled和timeout分别表示是否启用该工具以及超时时间\n还有一个transportType，这也是非常重要的，有两种选择，就是先前于传输层实现细节提到的，但是网上具体的MCP Server提供的Server Config模版很多时候不自带这个选项，可以自行指定\n\nstdio\nsse\n\n&#123;  &quot;mcpServers&quot;: &#123;    &quot;fetch&quot;: &#123;      &quot;disabled&quot;: false,      &quot;timeout&quot;: 60,      &quot;command&quot;: &quot;uvx&quot;,      &quot;args&quot;: [        &quot;mcp-server-fetch&quot;      ],      &quot;transportType&quot;: &quot;stdio&quot;    &#125;  &#125;&#125;\n\n然后就可以使用了\n\n同理，在Claude Desktop中\n找到对应的位置，打开config.json （claude_desktio_config.json）\n\n复制写入必要内容，然后重启Claude Desktop即可使用。\n&#123;  &quot;mcpServers&quot;: &#123;    &quot;fetch&quot;: &#123;      &quot;command&quot;: &quot;uvx&quot;,      &quot;args&quot;: [        &quot;mcp-server-fetch&quot;      ],      &quot;transportType&quot;: &quot;stdio&quot;    &#125;  &#125;&#125;\n\n"},{"title":"公告(说明)","url":"/2025/05/09/announcement/","content":"本站仍处于建设过程中~内容的补充和校对会在假期进行~🥲\n如果有任何建议或意见，欢迎联系博主~🥰\n","categories":["announcement"],"tags":["announcement"]},{"title":"dify-introduction","url":"/2025/05/13/dify-introduction/","content":"dify部署与访问部署在——本地机特定端口只要在局域网下就能直接访问(包括编写好的AI应用提供的服务)(检验过) (个人测试时没有受到本地防火墙的干扰，但若出现，可以手动关闭)以浙大为例，只要连在同一个无线网下就可以，校外用浙大RVPN应该能实现同样的效果(还没检验过)\n\n反正找到能访问本地机的IP地址即可，如果没有上述红框圈起的地址或地址无效。可以使用ipconfig寻找当前机器的无线网络适配器的IP地址。\n其他人只需要在浏览器网址栏输入 [该地址]/signin即可访问。如果不行的话有可能会是本地有多个web服务(产生冲突，不能自动索引？我猜的)。不管怎么说，可以使用netstat命令观察本地所有对外开放的端口，无非全部检查一遍，在明确dify服务监听的特定端口之后，在浏览器网址栏输入[该地址]:[端口号]/signin应该就万无一失了。\n\n一些前置知识ollama是什么？\n玩具，并不重要，事实上现在有很多能够提供模型API的平台了，比如https://openrouter.ai/models  。包括我们也可以使用模型官网提供的API或者其他各种模型管理平台，对于dify来说，只需要安装对应插件就能提供支持\nollama是一个模型管理平台，可以用来管理模型，包括模型的上传、下载、部署、监控等。本身并不是模型部署在本地机上。开源~~\n模型后面跟的B是什么？B:billion，表示10^9，表示模型的可训练参数的数量\ndify是什么？dify是一个开源的AI应用开发平台我们可以将它作为脚手架进行AI应用的开发\n\n减少造轮子的工作\n可以以较高地效率完成复杂创意工作流的设计\n自由度很高，可以diy (比如不满意它自带的知识库检索，可以外接RAGflow API)\n有别人写好的插件或模版可以直接套用\n方便团队协作\n方便调试\n\n模型从哪来？右上角用户-&gt;设置-&gt;模型供应商 (在右上角系统模型设置还能看到我们的默认模型)dify支持了很多了模型供应商——本质上是通过插件控制了接口，非常简单，安装插件即可增加支持\n\n如何添加dify上可用的模型？如果是我们本地GPU服务器提供的模型\n\n在ollama模型供应商里选择添加模型\n直接填写提供ollama服务的服务器的ip地址和端口号就好(虽然这也是ollama的安全隐患，但作为玩具，无所谓了)\n之后填写该服务下提供的一个 模型名称 即可。若是通过检验，就会成功加入\n\n\n\n\n\n如果是自己本机跑的模型\n\n那就填自己本机提供服务的ip地址和端口号，原理上是类似的。本机上的模型不一定要依赖于ollama，还可以考虑Xorbits Inference\n\n其它闭源模型直接填API key即可但是不小心被别人拿去用了可能会狠狠烧钱\n\n不确定添加闭源模型之后别人能否直接使用该模型进行编排(可以测试)\n但其他人至少可以通过无休止地运行你编排的工作流而间接地燃烧你的API token\n\n\n工作室创建AI应用的地方\n导入DSL文件是直接导入现有应用(别人做好的)从应用模版创建可以从现有模版创建，站在巨人的肩膀上XD\n\n创建空白应用就是正常地自己创建有四种应用类型，需要关注的主要是三种，现2025年已经拓展到5种\n\n聊天助手\nAgent\n文本生成应用\nchatflow\n工作流\n\n三种应用类型区别聊天助手就是字面意思：一个可以和你进行多轮对话的机器人Agent可以简单理解为聊天助手的升级版——可以调用工具辅助完成任务工作流就是一个流程图，在其中我们可以编排任务的处理方式，可以联合调用多个模型、工具，可以实现自动化的任务处理。\n三种应用类型各有优劣\n\n聊天助手：可多轮对话，支持基础编排和工作流编排，后者的情况下可以应用为可多次根据用户输入处理任务的工作流\n\n工作流(无敌)：静态编排，稳定性强，但一次性工作，结束了就结束了。但是可发布为工具被Agent调用\n\n如果使用新增的chatflow，那么可以循环对话，感觉已经没有痛点了……工作流is all you need\n\n\nAgent: 动态编排，灵活性强。理论上来讲，我们期望Agent做的事是：利用自身的推理能力，将任务分解成多个子任务或者一个任务流程，并利用工具分步解决各个任务，最后输出结果。实际上我们期望的就是它自己去实现一个工作流的编排，并执行。容易知道这对模型推理能力的要求是很高的，就我们目前拥有的模型实例而言，Agent的推理能力还不够强，所以不能太依赖模型本身的推理能力去完成任务。通过在提示词中给出工作流程的示例或规定、将能完成特定任务的工作流作为工具封装好，从一定意义上可以减轻这个问题。\n\n但个人认为这个行为本质上还是通过降低了编排的动态性，增强其静态性，让Agent更趋于工作流，从而降低灵活性，提高稳定性，降低对模型推理能力的要求。\n\n\n\n关于提示词可以发现需要提示词的地方右上角都有一个AI自动生成。如果自己不想写的话可以利用那个功能快速写一段提示词。\n注意提示词的生成能力取决于你在设置的时候设置的推理模型\n\n不过坦白说可以直接这样生成提示词——https://www.aipromptgenerator.net/zh?utm_source=ai-bot.cn\n\n关于编写提示词下面是一些实践经验但不一定靠谱，靠谱的可以看这——https://www.promptingguide.ai/zh\n很多时候，只需要在最后面打上几个换行，然后用 ## 开头写个小标题，然后在下面补上几句说明就行了。使用 ## 打小标题的目的是提供更清晰、更结构化的说明，这有助于模型的理解。\n\n例如：## 输出说明使用中文输出\n## 输入说明用户输入为\n\n在提示词等地方需要插入或者绑定变量的话只需要用 { {  } } 框起来就行\n在工作流环境中给大模型配置提示词的时候可以发现有三种类型\n\nSYSTEM: 系统提示词——为对话提供高层指导(比如它要干什么，它的约束是什么输出格式等)\nUSER: 用户提示词——向模型提供指令、查询或任何基于文本的输入()\nASSITANT: 助手提示词——理论上是提供基于用户消息的模型回复但在实际中目前似乎并未发现三者除了名字外有什么应用差异((?\n\n关于工作流的编排核心操作\n\n鼠标右键可添加节点或注释\n鼠标左键选中连接线之后按backspace键可以删除连接线\nctrl+c, ctrl+v可以复制粘贴节点\n在连接线上按 + 图标可以直接插入一个节点\n可以添加的节点除基础节点外还有工具(可以把工作流理解成静态编排的Agent，也是有工具调用能力的)\n\nchatflow和workflow的节点可能会稍有不同，但本质上是一致的。\n\n在开始节点，可定义输入变量。包括文本输入和文件输入等\n\n问题分类器，可利用模型对输入变量进行检验，完成问题的分类，决定工作流的走向\n\n变量聚合器，可将多个输入变量聚合成一个变量，方便后续进行统一处理(但是类型匹配是个讨厌的问题)\n\n结束节点，结束工作流，记得配置输出变量\n\n\n完成配置之后记得发布和更新，不然不会生效在发布选项卡里还有发布为工具，相当于可以对工作流进行封装，只暴露输入变量和输出变量的接口。\n反正结果大概就这样，挺图形化的，挺简单的\n\nAgent的使用提示词原理同上，可以AI生成，也可以人工输入。用自然语言描述即可。结构化的提示词is preferred。\n可以调用工具\n特别注意：写完之后记得发布更新！！！不然退出之后什么都不会留下(2024&#x2F;12&#x2F;13)\nAI应用创建完成之后按运行，在任何地方都可以使用。直接把链接发给别人，别人就能用。(前提应该是能访问到dify部署的机子，在浙大局域网下应该就能访问)\n坦白说，dify在创建应用模版的时候所谓的Agent并不是最符合个人对Agent的印象的，感觉实际上工作流或者说chatflow可实现的智能化程度更高？\n好吧其实也只能说是工作流上限更高，但可能做的不好地话会偏静态\nAgent应该是可以动态调用工具。\n\n知识库的建立可以上传各种文本。实际上文本本身没有特殊要求——即便更加结构化的数据有利于AI更好地建立知识库。随便写一段txt都可以导入建立知识库。但是杂乱的数据可能对嵌入模型的能力需求更高。\n分段设置自动分段与清洗，挺好的。对于一般的数据可以使用它。如果是自己手写的知识库，推荐使用自定义。比如定义 ## 为分段标识符，就基本可以保证实现完美的分段效果。\n在索引方式方面。选择高质量意味着用模型去进行文本嵌入，否则使用离线的文本索引方式(dify自带的)。\n检索设置涉及到重排序问题，需要rerank模型。\n后面新增的父子分段可能更好地捕捉上下文关系，见仁见智吧\n调试打开对应的AI应用，点到日志页面，可以追踪运行结果，调试还是比较方便的\n\n工具工具是Agent的灵魂，不必我多说了叭(\n除了自己编排工作流发布成工具外\n在dify的市场界面也可以使用很多人做好的工具\n\nMCPMCP协议的出现极大地赋能了AI工具的开发，现在市面上已有上万MCP server\n\n其中一个网站：https://mcp.so/\n\n虽然尚未尝试，但注意到dify的拓展中已经出现了对MCP相关的支持，如果能够使用MCP赋能，那么dify的边界将会更加强大\n—— 或许只要不涉及到底层算法层面的革新，dify已经能够满足对AI应用开发的所有需求(bushi)\n\n"},{"title":"test2","url":"/2025/05/07/test2/","content":"","categories":["test"],"tags":["test"]},{"title":"test1","url":"/2025/05/07/test1/","content":"StartDo something here\n","categories":["start","nothing"],"tags":["test"]},{"title":"test3","url":"/2025/05/07/test3/","content":"","categories":["test"],"tags":["test"]}]