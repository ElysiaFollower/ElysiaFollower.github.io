[{"title":"重读经典：Word2Vec如何通过“简化”撬动了整个NLP世界？","url":"/2025/08/25/@mikolov2013EfficientEstimationWord/","content":"摘要: 2013年，一篇名为《Efficient Estimation of Word Representations in Vector Space》的论文横空出世，它提出的构成Word2Vec核心思想的CBOW和Skip-gram架构，彻底改变了我们让机器理解语言的方式。本文将带您重温这篇里程碑式的著作，探讨其核心思想：为何一个看似更“简单”的模型，反而能发现语言中令人惊叹的深层结构？\n原文链接：https://arxiv.org/abs/1301.3781\n注：为了无障碍理解这篇论文，阁下可能需要先理解文末 前置知识 中提到的一些基本概念。\n问题的根源：昂贵的“理解”在Word2Vec出现之前，自然语言处理（NLP）领域长期面临一个难题：如何让计算机理解词语之间的关系？传统方法如One-hot编码，将每个词视为一个独立的符号，这使得“国王”和“女王”在模型眼中毫无关联。\n\nas these(notion of words) are represented as indices in a vocabulary\n\n虽然当时先进的神经网络语言模型（NNLM）已经可以通过“分布式表示”（Distributed Representation）将词语学习为低维、稠密的向量（即词向量），并在一定程度上捕捉到语义相似性。但这些模型通常包含一个或多个复杂的非线性隐藏层。这个隐藏层是模型强大表达能力的核心，却也成为了巨大的计算瓶颈。其复杂度，特别是Hidden Layer的计算（N×D×H part），使得在数十亿词级别的海量数据集上训练模型变得不切实际。简单来说，当时的“理解”非常昂贵，以至于我们无法用足够多的数据去“喂养”它。\n\nThe main observation from the previous section was that most of the complexity is caused by the non-linear hidden layer in the model\n\n解决方案： 大道至简的两个模型面对上述瓶颈，Mikolov和他的同事们提出了一个颠覆性的思路：能不能用一个更简单的模型，来换取在更大规模数据上训练的能力？于是，他们大胆地移除了计算成本高昂的非线性隐藏层，从而极大地降低了计算复杂度。答案就是本文提出的两个核心架构：CBOW 和 Skip-gram。基本可以认为，这两个模型就是后来广为人知的Word2Vec。\n\n\nContinuous Bag-of-Words (CBOW)\n\n\n任务：根据上下文（周围的词）来预测中心词。\n做法：将目标词周围N个词的向量从投影矩阵(一个巨大的查询表，每一行即为一个词的向量)中取出，然后直接对它们进行求和平均，形成一个汇总的上下文向量，并用这个向量去预测中心词 。因为忽略了上下文的词序，所以被称为“词袋”（Bag-of-Words）模型。\n特点：多对一的预测，训练速度更快，对高频词效果更好。\n\n\nContinuous Skip-gram\n\n\n任务：与CBOW相反，它根据当前的中心词来预测其上下文。\n做法：将中心词的向量作为输入，去预测其前后一定范围（窗口C）内的多个上下文词语。并且通过将窗口的大小设计为固定范围的随机数，便可实现统计意义上的对远端词语进行更少采样，从而给予近处上下文更高的权重，这或许可以被看作是一种朴素的、非动态学习的“注意力”思想的雏形。\n特点：一对多的预测，训练时间更长，但能学习到更好的低频词表示，在大型语料上表现通常更优。论文的实验结果（如Table 3）也证实了这一点，Skip-gram在语义准确性上以55%对24%的巨大优势超过了CBOW。和CBOW不同，Skip-gram的训练模式可能会有些难以理解，容易产生歧义，具体讲解可见 Skip-gram训练机理\n\n这种简化带来的好处是惊人的。计算复杂度从NNLM的Q = N×D + N×D×H + H×V 急剧下降到CBOW的 Q = N×D + N×log_2(V) 和 Skip-gram的 Q = Cx(D + D×log_2(V))。正是这种效率上的巨大飞跃，使得在一天之内处理完16亿词的语料成为可能 ，也为接下来那个“魔法般”的发现奠定了基础。\n注：这里提到的Q基本上可以理解为模型一次预测所涉及的参数量; \n\nSimilar to [18], to compare different model architectures we define first the computational complexity of a model as the number of parameters that need to be accessed to fully train the model.\n\n注：此外，V到log_2(V)的变化是通过使用词汇的二叉树表示实现的，也就是说这对于NNLM的复杂度优化也是有用的，但是优化完之后，性能瓶颈在NxDxH, 仍然没有消失。\n\nWith binary tree representations of the vocabulary, the number of output units that need to be evaluated can go down to around log2(V ). Thus, most of the complexity is caused by the term N × D × H.\n\n惊人的发现：向量空间中的线性关系如果说高效的模型是这篇论文的“肌肉”，那么它揭示的向量线性关系就是其“灵魂”。论文最令人振奋的发现是，通过上述简单模型在海量数据上训练出的词向量，不仅仅是让“猫”和“狗”在空间中彼此靠近，更是捕捉到了词语之间丰富、可量化的类比关系（Analogy）。\n这种关系可以通过简单的向量代数运算来揭示。最经典的例子莫过于：这个等式石破天惊，它表明词向量空间中蕴含着抽象的语义维度，例如“性别”、“皇室”等。从“国王”的向量中减去“男人”的向量，相当于抽离出“男性”这个概念，保留了“皇室权威”等特征；再加上“女人”的向量，就将这个“皇室权威”的特征赋予了女性，最终得到的向量在空间中将接近“女王”对应的向量。在这个空间中，词向量的距离可以表示语义的相似度。\n为了系统性地验证这一发现，作者团队专门构建了一个包含语义(Semantic)和语法(Syntactic)类比问题的综合测试集Semantic-Syntactic Word Relationship test set。示例：\n\n语义类比: Athens is to Greece as Oslo is to? (Norway)\n语法类比: apparent is to apparently as rapid is to? (rapidly)\n\n注: 在实操上，可以通过上述向量运算得到结果向量，最后在整个词汇表的向量中，寻找与这个结果向量距离最近的词向量，这个词就是模型的答案——论文中使用的距离度量为“余弦距离”。在下以为，若所有词向量都预先进行归一化（使其长度为1），那么计算成本高昂的余弦距离就可以被简单的“向量点积”所替代。在单位球面上，按点积大小排序等同于按欧氏距离排序，但计算效率更高。ps: 单位球面上，\n实验结果（论文中的Table 3, 4; 6）雄辩地证明，CBOW和尤其是Skip-gram模型，在这类任务上的准确率远超当时更复杂的NNLM和RNNLM模型，同时训练成本大幅降低 。准确率：训练成本：\n历史回响：前沿性它被推翻了吗？\n思想从未被推翻，反而成为了基石。 “将词语映射到稠密向量空间中，通过无监督学习捕捉其语义”这一核心思想，已经成为整个现代NLP的奠基性观念之一。\n\n它还是最前沿吗？具体技术已演进，但思想永存。\n\nWord2Vec的局限在于它生成的是静态词向量(训练得到一个固定的词向量查询表)，对于一个token只有一个表示，无法处理一词多义问题（例如”bank”的“银行”和“河岸”两个含义共享同一个向量）。而当代的SOTA（State-of-the-Art）技术，如BERT、GPT等基于Transformer的模型，生成的是动态的、上下文相关的词向量。它们能根据句子语境为同一个词生成不同的表示，极大地提升了语义捕捉的精度。大体上应该可以理解成：输入句子之后，句子中的所有词首先经过一层类似于Word2Vec的初始嵌入，然后在经过注意力机制处理后，再输出每个词最终的词向量\n\n同时，CBOW由于其BOW的模型，缺乏对位置的建模； Skip-gram的位置概念也非常模糊，并未显式建模。\n\n此外，word2vec模型本身并未提供直接获取整个句子向量的有效方法，通常采用的“词向量平均”策略会丢失重要的语序和语法信息。\n\n然而，即便在今天，Word2Vec因其高效、轻量的特性，仍在学术研究和工业界中扮演着重要角色，甚至就连它被诟病的无法应对一词多义的问题有时也是一种优势——因为它可以得到一个稳定的向量训练结果，若要知晓某个词的向量，在向量表(投影矩阵)中直接查询即得。\n\n\n杂谈《Efficient Estimation of Word Representations in Vector Space》是一篇充满“反直觉”智慧的论文。它告诉我们，有时最优雅的解决方案并非来自更复杂的模型，而是来自对问题本质的深刻洞察。通过极致的简化，Word2Vec将计算力从复杂的模型结构中解放出来，投入到对海量数据的学习中，最终发现了语言本身蕴含的、令人着迷的数学之美。这不仅是一次技术的胜利，更是一次思想的胜利。\n一些可能容易迷惑的点：最终产物虽然Word2Vec在训练时的确利用了上下文，但请注意，训练的最终产物是一个固定的查询表（即投影矩阵 W）。训练结束后，无论“bank”出现在什么新的句子中，我们去使用它的词向量时，做的动作仅仅是查表——从矩阵W中把“bank”对应的那一行向量取出来。这个向量是一次性生成、全局固定的，所以它是“静态”的、“上下文无关”的。上下文信息在训练时被“蒸馏”进了这个固定的向量里，但在使用时，新的上下文不起作用。\n实际上这种方式训练出来的是一个既能进行嵌入又能进行预测的模型——正是因为有预测的能力，所以才能计算损失进行反向传播，从而不断调试投影矩阵，进而得到较好的词向量表示，也就得到了一个较好的嵌入模型。这或许也算是这类自监督学习（Self-supervised Learning）模型的训练哲学吧。我们并不真正关心模型预测得有多准，这个“预测任务”只是一个“借口”。我们真正的目标是，在完成这个任务的过程中，强迫模型学习到有意义的中间表示——也就是那个高质量的“投影矩阵”（词向量）\n共享投影层的说法论文在介绍模型架构时提到的共享投影层(using a shared projection matrix)可能会让人感到困惑。在下认为，这里的共享主要指的是对于上下文窗口的不同位置，使用是同一个投影矩阵进行映射；对比的是上下文窗口的每个位置分别维护不同的投影矩阵。除此之外，在CBOW中，作者还做了一个更激进的共享——投影后所处的空间位置也进行共享：输入的N个词向量在查找到之后，将被直接平均（Average）成一个单独的D维向量，空间位置信息被彻底抹除；对比在NNLM中，查到的向量会被拼接成N*D维的长向量\n\nNote that the weight matrix between the input and the projection layer is shared for all word positions in the same way as in the NNLM.\n\nSkip-gram训练机理要理解Skip-gram，首先需要明确其核心精髓——它不追求单个预测的精准度 预测任务只是一个“借口”，我们真正的目标是在这个过程中“锤炼”出优秀的词向量\nCBOW是将多个上下文单词加起来，然后用聚合后的向量一次性去预测中心词，每次训练就聚焦一次：目标词和使用聚合向量预测目标词的概率而Skip-gram, 则从中心词出发，和上下文窗口C中的所有词分别组成一个独立的训练样本对，每次训练就聚焦C次：这一次的目标词和使用中心词向量预测该目标词的概率例如：假设我们的句子是I am a fool, 窗口大小C=1, 当前处理的中心词是a则形成两个独立的训练样本对：\n\na -&gt; am\na -&gt; fool所以模型会训练两次，一次输入a，使用预测出am的概率计算损失；一次输入a，使用预测出fool的概率计算损失在使用层级Softmax的时候损失计算可能没有这么简单，但本质不变在反向传播的时候，不仅会调整输出层的权重，更重要的是也会调整a的词向量，从而不断拉扯a的词向量，让它更符合上下文语境\n\n计算复杂度论文中给出的计算复杂度公式乍一看可能有些吓人。但根据论文定义，它估算的主要是训练时需要访问的参数数量，理解起来并不复杂。下面我们进行简要拆解：\n\nNNLM: Q = N×D + N×D×H + H×V\n\n\nN×D：这部分代表从输入层到投影层的计算。将N个上下文单词（通常是one-hot）通过投影矩阵映射为N个D维向量，再拼接成一个N×D维的向量。因为涉及到了投影矩阵中的N行，所以对应参数量为N×D\nN×D×H：这是计算瓶颈，代表从投影层（N×D维）到非线性隐藏层（H维）的全连接计算。在这篇论文的语境下，应该是默认其所对比的NNLM为仅拥有单层非线性隐藏层的经典模型\nH×V：代表从隐藏层（H维）到巨大无比的输出层（V维，V是词汇表大小）的全连接计算。\n\n\nCBOW: Q = N×D + D×log₂(V)\n\n\nN×D：与NNLM类似，查询N个上下文单词的D维向量。\nD×log₂(V)：模型移除了N×D×H的隐藏层。将N个词向量平均后得到仅仅一个D维向量，直接用它来预测输出。同时，输出层使用层序Softmax（Hierarchical Softmax）这种技巧，将原本D×V的计算量降维打击至D×log₂(V)，因为它只需要在一个二叉树（深度约log₂(V)）上做一系列二分类判断，而不是在全部V个词上计算概率。所以这个技术对于NNLM同样适用，理论上上面NNLM的复杂度也可以优化，但是不影响CBOW和Skip-gram仍然远比它快。\n\n\nSkip-gram: Q = C×(D + D×log₂(V))\n\n\nD：输入只有一个词，查询其D维向量。\nD×log₂(V)：用这个D维输入向量，去预测一个上下文单词。与CBOW一样，也用了层序Softmax优化。\nC×(...)：因为Skip-gram模型需要对一个输入词，预测其周围的C个上下文单词，所以上述过程需要重复C次。\n\n一言以蔽之：Word2Vec的效率革命，主要来自（1）砍掉N×D×H的隐藏层 和（2）用log₂(V)复杂度的层序Softmax替换V复杂度的普通Softmax。\n关于层序Softmax的使用，感兴趣的话可以作者参考引用的这几篇论文\n\nT. Mikolov, A. Deoras, D. Povey, L. Burget, J. Cˇ ernocky ́. Strategies for Training Large Scale Neural Network Language Models, In: Proc. Automatic Speech Recognition and Understanding, 2011.\nA. Mnih, G. Hinton. A Scalable Hierarchical Distributed Language Model. Advances in Neural Information Processing Systems 21, MIT Press, 2009.\nF. Morin, Y. Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS, 2005.\n\n前置知识\n词的表示方法：从One-hot到分布式表示\n\nOne-hot：想象一个词典有10万个词，那么“apple”这个词可能被表示成一个10万维的向量，其中只有对应“apple”的位置是1，其余全是0。这种方式无法体现词与词之间的关系 。\n分布式表示 (Distributed Representation)：与One-hot不同，它用一个稠密的、低维度的向量（例如300维）来表示一个词。向量中的每一个维度都代表了词语的某种潜在特征。这篇论文的核心就是如何高效地学习这种表示。\n\n\n全连接层\n\n一个全连接层包含一个权重矩阵W和一个偏置向量b。当输入一个向量x时，输出为y = Wx + b。\n\n\n神经网络语言模型 (NNLM) 的基本结构\n\n阁下只需要大致了解它通常包含输入层、投影层、隐藏层和输出层。\n其中投影层的概念可能会让人有点陌生，但对于理解本文思路又至关重要，所以这里在下简要介绍基本原理\n您可以把它想象成一个巨大的查询表。输入层的一个词就像一个开关，正好选中表中的某一行。这一行，就是一个低维、稠密的向量，它就是那个词的分布式表示。因此，这个投影层矩阵本身，在训练结束后，就成为了我们最终想要的“词向量词典”。\n\n\n\n\n向量空间与相似度\n\n当词语被表示为向量后，我们就可以在多维空间中计算它们之间的距离（如余弦距离）。距离越近，代表这两个词语的语义或用法越相似 。\n\n\n\nReferences[1] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n","categories":["AI","basis"],"tags":["AI","basis","Word2Vec","paper","impressive"]},{"title":"公告(说明)","url":"/2025/05/09/announcement/","content":"本站仍处于建设过程中~内容的补充和校对会在假期进行~🥲\n\n说是要在假期进行，但转眼间假期都要结束了…正好近期没什么事，外加正好在做文献整理，便稍微建设一下叭(((\n\n如果有任何建议或意见，欢迎联系讨论~🥰\n小站引用了许多来自网络的图片，能力有限，无法全部找出原作者orz😭如需注明作者请联系博主,侵删🫶    \n","categories":["announcement"],"tags":["announcement"]},{"title":"使用Git进行代码管理的经验谈","url":"/2025/08/30/Git-experience/","content":"这篇文章主要记录一些Git工具的使用方式，因为Git提供了很强大的功能，但是大部分功能在大多数情况下都不会用到，故在此简要记录方便未来作为速查表使用(使用ctrl+F可在浏览器快速查找关键词)。除此之外也记录一些在基于Git的开源社区如GitHub上观察到的一些社区规范或者说暗语, 在进行项目代码管理或参与开源贡献的时候可以作为参考。本文属于是不完全的梳理，所谓”经验谈”的意思其实就是当咱遇到了再进行相关的整理啦XD会持续更新的orz如有不恰当的地方或者想要补充的，欢迎在评论区交流 (^_−)☆\n使用Git拉取和推送最常见的自然是git pull和git fetch，但是个人更喜欢先fetch, 再merge。因为事实上，git pull其实是一个组合件，它会先调用git fetch, 然后再根据情况选择调用git rebase或者git merge[4][5] (或许大多数时候，它调用的是merge，基本可以认为它会调用merge，但只要咱把它拆开来用，把一切的掌控权拿在自己手中，不就不用纠结这个问题了喵(￣▽￣)~*)\n推荐流程:\n\ngit fetch &lt;远程仓库别名&gt; &lt;分支名&gt; 拉取远程仓库的指定分支的最新内容到本地，但不会直接合并, 先看看都发生了什么新的更改, 让自己拥有充分决策权\ngit merge &lt;远程仓库别名&gt;/&lt;分支名&gt; 将您看中的分支合并到当前分支，若出现冲突，则需要解决冲突，并提交\ngit push &lt;远程仓库别名&gt; &lt;分支名&gt; 将本地指定的分支上传到远端并和同名分支进行合并\n\n\n客观地说，最后一个部分可以改为&lt;本地分支名&gt;:&lt;远程分支名&gt;, 从而和指定分支进行合并，但是这真的有必要吗？中肯地说，在每次push之前，都应该先fetch远端所有必要的更改并在本地merge解决冲突之后，再进行push，所以最终分支名大抵是相同的。\n\n\n管理远程仓库git remote系命令可用于管理与当前仓库链接的远程仓库，具体用法如下：\n$ git remote add &lt;别名&gt; &lt;仓库URL&gt;\n该命令可以添加一个远程仓库，其中’仓库URL’替换为目标仓库的URL，即xxx.git, ‘别名’替换为任意名称，如’origin’/‘upstream’eg: git remote add upstream https://github.com/ElysiaFollower/ElysiaFollower.github.io.git在下认为这是这个分支最重要的一个命令\n至于添加远程仓库以后？那就基本可以完全抛却负担，像正常使用git一样进行使用啦。因为当我们平时git clone一个仓库的时候，实际上只是git clone自动帮我们添加了一个远程仓库, 本质是一样的[2]。关于这个问题，其实下面这条命令正好可以进行一定程度的佐证:git remote -v可以查看当前仓库的远程仓库举个例子:\n(base) PS D:\\myblog\\blog_in_develop\\themes\\redefine&gt; git remote -vorigin  https://github.com/ElysiaFollower/hexo-theme-redefine.git (fetch)origin  https://github.com/ElysiaFollower/hexo-theme-redefine.git (push)upstream        https://github.com/EvanNotFound/hexo-theme-redefine.git (fetch)upstream        https://github.com/EvanNotFound/hexo-theme-redefine.git (push)\n这是在下fork的一个仓库，这里的origin仓库就是git clone时自动产生的，而upstream仓库则是在下手动添加的。可以看到，在git remote -v眼里，他俩是同一回事。理解到此为止，所以添加之后只需要像正常一样使用git push和git fetch进行代码管理即可。不过需要注意的是，git fetch命令的’完整版’其实应该是[2][3] :git fetch &lt;远程仓库别名&gt; &lt;分支名&gt;只是git clone的时候，同时生成了一些缺省时的默认配置，但是在管理多个远程仓库的时候还是全部显式声明更加所见即所得。\n\nWhen git fetch is run without specifying what branches and/or tags to fetch on the command line, e.g. git fetch origin or git fetch, remote..fetch values are used as the refspecs—​they specify which refs to fetch and which local refs to update[3].\n\nfetch如此，push也是同理的:git push &lt;remote别名&gt; &lt;branch&gt;\n查看远程仓库相关信息:git remote show &lt;remote别名&gt;\n删除远程仓库：git remote remove &lt;remote别名&gt;\n社区暗语从Fork到Pull Request(PR)不知道大家是否会经常听到PR这个词呀，在下第一次听到这个词的时候，还以为是peer review的缩写(死去的fds和ads课程回忆突然开始攻击我), 但是代入上下文语境总觉得有些违和。后来了解之后才知道在开源共享或者项目管理中，PR往往指的实际上是Pull Request，即拉取请求 —— 您向原项目/仓库的维护者提交请求，建议他们合并你所做的修改在我的观察中，PR实际上可以理解为: 存在一个仓库A，而我们的手里有一个仓库B，这是基于A的延伸，或者基于A的修改，我们在仓库B的git历史中存在某一个分支，就像正常分支合并一样，我们可以通过Pull Request将这个分支合并/merge到仓库A的指定分支上中。 \nPR的好处都有啥PR的意义尤其在于开源贡献。请设想这样一个场景: 阁下在网上冲浪的时候发现了一份写得很好的代码，或者自己在使用某个开源工具的过程中体验到了一些难处，作者一直没有解决这个问题于是你决定自己动手。但是当您兴致满满地git clone完原仓库，幸苦完成了修改之后，自信满满地键入git push origin xxx-new-feature，您发现这个操作失败了，然后您意识到——您根本没有这个仓库的提交权限！若阁下尝试去直接联系仓库的维护者，且不说对方是否能够及时回复，就算回复了，对方可能也会出于不想有人把代码搞得一团糟甚至删库跑路等顾忌而不愿轻易提供写权限。那么难道这就死局了吗？非也非也，聪明的程序员们发明了PR机制来解决这个问题。它有以下几个特点(根据在GitHub上的观察):\n\n即便是对仓库没有写权限的人也可以发起PR\n当阁下发起PR的时候，阁下可以为您的修改写一份声色俱茂的小作文，包括目的、动机、实现方法、效果和测试结果等\n仓库的维护者可以直接查看您的修改(diff形式)，如果认为合适就可以合并到原仓库中，完成合并则等效于真的在原仓库中开了这么一条分支，在开发后合并到特定分支\nPR拥有评论区，您可以和仓库的维护者或者其他人在这里展开充满激情的辩论。如果您认为他人说的有道理，您可以回到自己的分支中修改代码(GitHub的PR会自动更新最新提交内容-2025/8/19)，然后您就会惊奇发现，当时那条PR的commit记录自动延长了，阁下可以马上附上更新说明，在一个连续的历史上下文中完善您的这一次贡献\n\n考虑到PR拥有的这些强大的能力——特别是“小作文”和评论区的设计——实际上为代码审查提供了很大的方便。所以在下肤浅的认为，对于一些大型项目的大型改动来说，即便是对于拥有写权限的开发者来说，学习使用PR也是非常有意义的！虽然网上能搜到的一些协作开发的范式大都是不同人负责不同的分支，开发完了合并。或许这对于较为小型的项目来说更为方便，但是试想那位进行merge的人需要承担多大的心里负担呐。擅自将自己开发完的分支合并到别人的分支或者main分支上，说不定过段时间就会有一位悲惨的程序员发现自己原来能跑的代码跑不了了，于是悄悄地破防了(；´д｀)ゞ。如果联系项目负责人，让Ta进行审查合并，且不说联系过程怪麻烦的，而且整个审查并不是公开透明的，未来出现了问题也增加了维护的难度。看，这些问题通过PR机制都能得到很好的解决ヽ(￣▽￣)ﾉ\nFork实乃何物PR这么有用，那么怎么PR呢。这时候就不得不提Fork了。在下以为Fork+PR的组合拳，已经是进行PR的一种社区规范了。\nFork实际上就是将一个仓库复制到自己的账号下，形成一个复制仓库，这样您就有了这个项目的副本，可以随意进行修改，而不会影响到主项目。\n\n也就是说，除了要发起PR的分支，阁下甚至可以开几条属于自己的草稿分支，并且不用担心遭到同伴亲切的问候“ 您在做神魔(O_o)?? ”\n\n至于怎么发起PR，则只需要在GitHub web端进入Fork形成的仓库副本，在Pull Request栏发起PR请求即可。事实上于在下的印象之中，当已经commit了几条之后，Fork的仓库首页就会贴心地提示您”Compare &amp; pull request”[1]\n小技巧在Fork的仓库中，阁下可以通过git remote add upstream &lt;原仓库URL&gt;来添加一个上游仓库(对应的git操作请参考)，这样您就可以通过git fetch upstream来将原仓库的最新代码拉取到自己的仓库中并通过git merge upstream/branch_name合并了！(￣▽￣)~*这样的话，就算是团队内部进行开发或者修改自己使用中的工具，使用Fork仓库也完全无负担，反而更方便，自由度更高\n\n不仅可以无缝追踪原仓库进度\n还有自己的草稿区\n\n注: 一般使用upstream作为原仓库的别名\n\nCommit Message规范网上很有多关于commits规范的说法，有的显得很复杂。在下以为，重要的还是能不能把事情说清楚(看到阿里在文章中说”建议使用中文（感觉中国人用中文描述问题能更清楚一些）”, 笑罢觉得还是有些事情值得反思的，形式主义没有意义)，所以这里摘些个人最认可也认为是最重要的部分充当一下互联网的记忆叭(￣^￣) [6][7]\n最值得参考的大抵还是Angular规范[8]和后来在它的基础上发展出来的Conventional Commits叭[6]Angular范式：\n&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;&lt;body&gt;&lt;footer&gt;\n作用域scope，body和footer都是可选的；前面几个都挺有用的，相对而言footer的可有可无性略高\n然后类型这边在下稍微糅合了一下，但都是接受度比较高的类型:\n\nfeat: 新功能\nfix: 修复bug\ndocs: 文档\nstyle: 样式修改 —— 主要是代码样式修改，不影响运行的那种[6]\nrefactor: 重构，代码重构，逻辑重构\ntest: 测试, 例如添加、删除、修改代码的测试用例等。\nchore: 用于对非业务性代码进行修改，例如修改构建流程或者工具配置等\nbuild: 用于修改项目构建系统，例如修改依赖库、外部接口或者升级 Node 版本等\nci: 用于修改持续集成流程，例如修改 Travis、Jenkins 等工作流配置；\nperf: 用于优化性能，例如提升代码的性能、减少内存占用等；\nrevert: 用于回滚版本\n\nfooter里面的’BREAKING CHANGE‘是什么？—— 简单来说就是不兼容修改, 本次提交修改后将不兼容之前版本的API或者环境变量。建议写上Before和After，分别表示修改前的和修改后的版本。\n关于怎么写，学理论不如看几个例子感受一下(。-`ω´-) ;取自[8] :\nReferencesCoding Is Fun. 开源贡献：从 Fork 到 Pull Request（PR）. CSDN, 2024 ↩Git. 2.5 Git 基础 - 远程仓库的使用. git-scm, ↩Git. git-fetch - Download objects and refs from another repository. git-scm, 2025 ↩Runner_Jack. git fetch &amp; pull详解. 博客园, 2018 ↩Git. git-pull - Fetch from and integrate with another repository or a local branch. git-scm, 2025 ↩. convertionalcommits. , ↩阿里云开发者​. 如何规范你的Git Commit?. 知乎, 2025 ↩angular. AngularJS Git Commit Message Conventions document. github, ↩","categories":["Tools","code"],"tags":["Git","manual","code management"]}]