[{"title":"公告(说明)","url":"/2025/05/09/announcement/","content":"本站仍处于建设过程中~内容的补充和校对会在假期进行~🥲\n\n说是要在假期进行，但转眼间假期都要结束了…正好近期没什么事，外加正好在做文献整理，便稍微建设一下叭(((\n\n如果有任何建议或意见，欢迎联系讨论~🥰\n小站引用了许多来自网络的图片，能力有限，无法全部找出原作者orz😭如需注明作者请联系博主,侵删🫶    \n","categories":["announcement"],"tags":["announcement"]},{"title":"欢迎来到AIGC的魔法世界","url":"/2025/09/02/%E6%AC%A2%E8%BF%8E%E6%9D%A5%E5%88%B0AIGC%E7%9A%84%E9%AD%94%E6%B3%95%E4%B8%96%E7%95%8C/","content":"参考在线课程https://modelscope.cn/learn/1582?pid=1577 [1]\n特别鸣谢: 阿里魔搭ModelScope平台\n本文旨在对课程核心内容进行记录与提炼，并融入了一些个人的理解和思考。文中若有任何疏漏或理解不当之处，敬请各位读者不吝指正，共同交流探讨(^_−)☆\n\n鉴于文本生成技术已广为人知，本文将重点聚焦于当前发展迅速且形式新颖的非文本内容生成，主要涵盖图像、音频及视频生成三大领域。\n\n侵删\n原理生成模型的演化历程生成模型的本质是什么？\n生成模型的本质，在于将一个服从简单随机分布(如高斯分布)的噪声与一个引导条件作为输入，通过模型复杂的非线性映射，将其转化为一个服从**目标数据真实分布(复杂随机分布)**的输出，这些输出最终表现为我们所见的图像、音频或视频等形式。\n\n随机噪声：\n\n它是生成多样性的来源。对于一个确定的模型，如果仅仅输入固定的引导条件(如提示词)， 理论上其输出应该是恒定的，因为模型本身是确定性的张量运算(就算是dropout层，也只是在训练的时候其作用，不会影响测试时的确定性)。然而在实践中，即使用户输入完全相同的提示词，每次生成的结果却也各不相同。这是因为模型在每次生成时，都会引入一个用户无法直接感知的随机噪声(通常由随机种子seed控制)。这个初始噪声的微小差异，经过模型的放大和转换，最终导致了输出结果的多样性。\n\n\n引导条件：\n\n这是用户可以控制的，用于指导生成方向的输入，例如提示词、**参考图像 (如线稿) **等\n\n\n\n生成式模型vs检索系统：\n\n从概念上讲，生成式模型与检索系统存在一个有趣的类比。两者都能根据用户输入的引导条件 (如文本) 返回相关的图像。它们的核心区别在于输出空间的性质：检索系统的输出空间是离散的 (从有限的数据中选取，是空间中离散的点)， 而生成模型的输出空间是连续的。\n这种连续性正是基于一个核心理念：与现代NLP模型类似，先进的图像生成模型同样在名为 “潜在空间(Latent Space)” 的高维向量空间中运作。 无论是输入的文本提示词，还是生成过程中的图像特征，都会被编码为这个空间中的向量(Embedding)。在这个空间中，语义相近的概念在空间位置上也相互靠近。关于潜在向量空间的理解，最早可以追溯到Word2Vec的词向量，可见这篇文章的解释\n因此，生成模型可以被视为一种平滑的检索。举个例子：它不仅能够找到代表程序员和猩猩的向量区域，更能在这些区域之间进行插值，从而创造出一个在训练数据中从未出现过，但语义上融合了两者特征的全新向量，最终解码为**程序“猿”**的图像。而离散的检索系统只能输出向量数据库中已有的离散点。\n生成式模型的演变\n如果要深入学习这个领域的话，或许可以参考这个时间脉络去进行学习。毕竟论文作者很多时候都默认读者知道一些前置的模型或概念，若是不提前了解可能会在理解上遇到一些困难\n\n当前主流的模型为 扩散(Diffusion)模型 \n其基本原理可以直观地理解为：从一张纯粹的噪声图开始，多步迭代，逐步去噪，在引导条件的指引下，最终“还原”出一张清晰的图像\n因为去噪过程实际主要依赖的是对噪声的预测能力，所以训练过程与生成过程恰好相反：向真实图像逐步叠加噪声，让模型在每一步都学习如何预测并移除被添加的噪声\n\n\n\n实践如果手上缺少算力的话可以在魔搭社区的AIGC专区玩玩 https://modelscope.cn/aigc/home\n如果有算力并且比较爱折腾的话，可以考虑使用魔搭团队开源的DiffSynth-Studio引擎[2]\n模型推理文生图文生图是最基本的生图模式，也是应用最简单，普及最广泛的一种生成模式。\n只需要用户输入一段文本形式的提示词，模型就能够生成一张相关的图像。\n提示词写作技巧前面我们已经提到，用户输入的提示实际上是对模型的一种引导条件，所以很多时候我们需要通过提示词对图像生成的内容进行引导，提示词的质量直接或间接地决定了生成图像的质量。容易理解，为了更好地指导模型，我们可能需要一些摄影的技巧；可能需要一些构图的知识   不过这大抵上属于是锦上添花，即便不懂摄影或构图，只要有审美细胞，都能进行调整\n关于写作提示词的具体技巧(范式)有\n\n提示词反推 —— 已经有了一张图，可以在这张图的基础上反推出提示词\n提示词润色 —— 更精细的描述\n负向提示词 —— 不要什么\n翻译\n因为有些图像模型，并不支持中文输入，但实际上翻译这一步有些时候可以省去。虽然很多时候使用英文提示词往往比使用中文提示词表现更好，但是我非常认可一个观点——概念优于语言。与其纠结prompt的语言，不如思考如何对AI描述清楚你的需求 [3]\n\n\n\n但事实上，我们可以让大语言模型来生成提示词，然后再自行调整\n可以参考的工具有很多[4]：\n\nLearning Prompt: 老是全权依赖工具也不是回事，可以在这里学些提示词设计经验\n\nPromptPort: 有很多现成的Prompt模版可以参考交流\n\nFlowGPT已经转型应用商店了，不太好搞。但是PromptPort体验还不错，提示词模版涉及各个领域，而且基本都有中英两版，可以阅读中文版快速理解然后复制英文版使用\n\n\n魔咒百科词典：魔法导论工具, 简单易用的AI绘画tag生成器\n\n在下第一次接触AI绘画的时候就听到有人喜欢使用“魔法导论”这样的说法来指代这个技术，这个比喻实在是有趣又贴切，因为基本的文生图AI绘画的的确确给人一种魔法的感觉，尤其是普通人第一次接触。只需要念出一连串咒语(关键词提示词)，再经由神秘的不可知域（模型黑箱）赐福，就能够创造出令人惊叹的“无上奇迹”（让一个普通人在极短的时间内创造出精密的图像）。同时它也似魔法一般，时而成功时而失败，不可预测。\n而这份“魔咒百科词典”，就是在这个魔法语境下的 提示词 模版啦。只是AI绘画的提示词往往都是关键词的形式存在的，所以正巧就变成了魔咒词典。\n\n\n\n可控生成技术除了提示词这种引导形式外，还有很多其他形式的引导条件，如图所示\n\n可控生成技术为生成过程提供了远超文本提示词的、精细化的空间和结构控制，在实践中已得到广泛应用，其中非常具有代表性的框架就是ControlNet [5]\nControlNet及其衍生技术，允许用户输入一张控制图(如线稿、深度图、人体姿态骨骼图等)，在保持原图结构、姿态或构图的基础上，由扩散模型进行内容的重绘和填充。\n不难想象，这是在生成领域至关重要的一项技术。因为人类多数时候难以仅用自然语言把需求完美地进行传递。在这项技术出现前，仅靠提示词进行图像生成，就像是在开盲盒，充满了随机性。\n例如在ComfyUI这类基于节点的工作流编排工具中，上述可控生成技术就是核心组成部分。一个节点的输出（如姿态检测结果）可以作为下一个生成节点的控制输入，从而实现复杂的多步生成任务。\nps: 从某种意义上来讲LoRA或许也算得上是一种控制，只不过是属于风格、角色或概念层面的控制，而非空间结构控制。\n图像融合技术\n试试融合一个篮球和一只鸡会怎么样(x\n工作流式应用搭建\n想象一下现实中画师的工作，大多数画师也不会一笔成稿，而是经历线稿到上色，人物形体到饰品等过程。让AI也像人类一样每次专注于一个任务可以取得更好的效果。\n模型训练LoRA全称Low Rank Adaptation\n一种在预训练的图像生成模型训练完成后，低成本拓展生成能力的技术[6][7]\n\n左侧蓝色矩阵为预训练模型原先训练好的参数矩阵，这个矩阵相对来说较为庞大(秩比较大)，如果直接进行微调，将会导致需要修改的参数量很大，训练量非常大。于是我们可以增加两个小矩阵，即图中右侧橘黄色的两个小矩阵，它们是分别是一个横长条的矩阵和一个竖长条的矩阵。假设原矩阵大小为 （ 同理）, 那么这两个小矩阵就是 , ， 于是两个小矩阵可以相乘消去r，得到一个和预训练参数矩阵维度完全一致的  的矩阵, 在进行预测的时候只需要将 作为新的模型权重参数，就可以实现对预训练参数的调整啦~\n\n\nps: 在实际应用场景中，往往还会给增量矩阵先乘上一个缩放因子, 从而控制LoRA的力道。\n\n因为r要远小于d，所以两个小矩阵的参数量 就要比 少上很多，训练时要调整的参数量也少不少，故而使得训练成本大大降低。\n图中B=0是为了保证在训练开始的时候，能够沿用原模型的能力, 因为训练开始的时候，BA为零矩阵，加到上不会发生任何事。\n但是注意，此时A绝不能为0！\n\n试想一下A和B同时为0会发生什么，梯度会彻底消失！简单想象一下，有一个表达式 ​,  如果 为0，那么对求导的时候，导数为，那么只要不为0，就有梯度，就有可能可以训练变化；但如果此时为0，则梯度为0，这个参数完全不参与最后的预测，也自然不会被训练到。反而反之，因为地位和地位相同，若=0，不为0，则对求导仍可训练。\n\nLoRA的普适性LoRA作为一种参数高效微调 Parameter-Efficient Fine-Tuning, PEFT), 其核心是“低秩适应”这一数学思想，因此它的应用范围远不止图像生成领域。基于对上述原理的观察，在下认为，任何模型，只要其包含权重矩阵，都可以利用LoRA进行微调，甚至可以有选择地对部分权重矩阵进行微调。ps: 有文章明确提到了对Transformer使用LoRA[8]。\n使用LoRA即便LoRA大大降低了成本，由于模型的参数量很大，使用个人消费级显卡仍然可能很难训练。o(╥﹏╥)o\n然天无绝人之路，我们还有魔搭社区的LoRA训练功能可以免费使用！\n而且阿里的工程师大佬说，他们是打算长期让这个功能免费的，本身便没有打算盈利，而是更多地希望推动技术的发展。(✧∇✧)\n其实就算不是自行训练LoRA，使用社区里别的大佬训好的LoRA试玩一下也是很有意思的。下面这是我使用社区中miratsu style LoRA模型，参考相关提示词简单生成的图像 —— 无恶意  ^(*￣(oo)￣)^ \n\n\n\n\n\n\n推荐模型图像生成工具\n\n魔搭 AIGC 图像生成专区：https://modelscope.cn/aigc/imageGeneration?tab=advanced\n开源项目 DiffSynth-Studio：https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/flux\n\n视频生成工具\n\n魔搭 AIGC 视频生成专区：https://modelscope.cn/aigc/videoGeneration\n开源项目 DiffSynth-Studio：https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/wanvideo\n\n音频生成工具\n\nCosyVoice：https://www.modelscope.cn/studios/iic/CosyVoice2-0.5B\nACE-Step：https://modelscope.cn/studios/ACE-Step/ACE-Step\n\nAPI服务\n\n百炼：https://bailian.console.aliyun.com/\n\n免费云计算资源（A10，24G显存暂时无法支持大模型推理和训练）魔搭 Notebook：https://modelscope.cn/my/mynotebook\n\n推荐技术路线针对于课程中提到的如何搓一个AI互动小游戏的推荐技术路线：\n\n训练图像生成模型的 LoRA（使用魔搭社区在线训练）\n\n编写脚本、分镜描述（手动编写或使用 LLM 生成）\n\n批量生成图像（使用魔搭 Notebook 调用 API）\n\n将部分图像转为视频（使用魔搭在线生成，或使用其他工具）\n\n生成配音（使用 CosyVoice 和 ACE-Step）\n\n整合资源\n\n\n前沿视频生成模型魔搭 AIGC 视频生成专区：https://modelscope.cn/aigc/videoGeneration\n音频生成模型ACE-Step模型\nCosyVoice\t\n多模态统一的模型架构\n虽然这种能够同时处理多个模态的“万能模型”听起来很美好，但事实上现阶段它就像是……啥都能干，但啥都不精 —— 博而不精。即模型虽然具备多种能力，但在任一单项任务上的表现，通常难以超越为该任务专门优化的顶尖模型。\n但是学术界和工业界仍然在乐此不疲地研究，仍然值得期待\n展望对齐训练与伦理道德\n因为AI的能力是从数据集上学习得来的，而数据集又来自于人类，这意味着AI不可避免地会学习到一些属于人类的偏好甚至偏见，导致其输出内容可能带有某些特定色彩。\n\nAI始终是服务于人类的工具\n其善恶之分\n依赖于执掌者之心\n\n额外课程图像、音频、视频、html实践案例Qwen-Image文生图、微调、编辑原理与实践魔搭社区小编 . AIGC多模态生成：从实践到原理（附：从0手搓一个AI互动剧情小游戏）. ModelScope, 2025 ↩ModelScope team. DiffSynth-Studio. GitHub, 2025 ↩. [探讨：中文提示词和英文提示词的效果区别](https://linux.do/t/topic/604467). LINUX DO, 2025 ↩知白守黑. Prompt资源精选|你想要的AI提示词都在这里了. 知乎, 2025 ↩Rocky Ding. 深入浅出完整解析ControlNet核心基础知识. 知乎, 2025 ↩Hu, Edward J., et al. LoRA: Low-Rank Adaptation of Large Language Models. arXiv preprint arXiv:2106.09685, 2021 ↩大师兄. LoRA（Low-Rank Adaptation）详解. 知乎, 2023 ↩Dreamweaver. 大模型的领域适配 —— Parameter-Efficient Fine-Tuning (PEFT). 知乎，2023 ↩","categories":["AI","AIGC"],"tags":["AIGC","Course Note","AI","practice"]},{"title":"使用Git进行代码管理的经验谈","url":"/2025/08/30/Git-experience/","content":"这篇文章主要记录一些Git工具的使用方式，因为Git提供了很强大的功能，但是大部分功能在大多数情况下都不会用到，故在此简要记录方便未来作为速查表使用(使用ctrl+F可在浏览器快速查找关键词)。除此之外也记录一些在基于Git的开源社区如GitHub上观察到的一些社区规范或者说暗语, 在进行项目代码管理或参与开源贡献的时候可以作为参考。本文属于是不完全的梳理，所谓”经验谈”的意思其实就是当咱遇到了再进行相关的整理啦XD会持续更新的orz如有不恰当的地方或者想要补充的，欢迎在评论区交流 (^_−)☆\n使用Git拉取和推送最常见的自然是git pull和git fetch，但是个人更喜欢先fetch, 再merge。因为事实上，git pull其实是一个组合件，它会先调用git fetch, 然后再根据情况选择调用git rebase或者git merge[4][5] (或许大多数时候，它调用的是merge，基本可以认为它会调用merge，但只要咱把它拆开来用，把一切的掌控权拿在自己手中，不就不用纠结这个问题了喵(￣▽￣)~*)\n推荐流程:\n\ngit fetch &lt;远程仓库别名&gt; &lt;分支名&gt; 拉取远程仓库的指定分支的最新内容到本地，但不会直接合并, 先看看都发生了什么新的更改, 让自己拥有充分决策权\ngit merge &lt;远程仓库别名&gt;/&lt;分支名&gt; 将您看中的分支合并到当前分支，若出现冲突，则需要解决冲突，并提交\ngit push &lt;远程仓库别名&gt; &lt;分支名&gt; 将本地指定的分支上传到远端并和同名分支进行合并\n\n\n客观地说，最后一个部分可以改为&lt;本地分支名&gt;:&lt;远程分支名&gt;, 从而和指定分支进行合并，但是这真的有必要吗？中肯地说，在每次push之前，都应该先fetch远端所有必要的更改并在本地merge解决冲突之后，再进行push，所以最终分支名大抵是相同的。\n\n\n管理远程仓库git remote系命令可用于管理与当前仓库链接的远程仓库，具体用法如下：\n$ git remote add &lt;别名&gt; &lt;仓库URL&gt;\n该命令可以添加一个远程仓库，其中’仓库URL’替换为目标仓库的URL，即xxx.git, ‘别名’替换为任意名称，如’origin’/‘upstream’eg: git remote add upstream https://github.com/ElysiaFollower/ElysiaFollower.github.io.git在下认为这是这个分支最重要的一个命令\n至于添加远程仓库以后？那就基本可以完全抛却负担，像正常使用git一样进行使用啦。因为当我们平时git clone一个仓库的时候，实际上只是git clone自动帮我们添加了一个远程仓库, 本质是一样的[2]。关于这个问题，其实下面这条命令正好可以进行一定程度的佐证:git remote -v可以查看当前仓库的远程仓库举个例子:\n(base) PS D:\\myblog\\blog_in_develop\\themes\\redefine&gt; git remote -vorigin  https://github.com/ElysiaFollower/hexo-theme-redefine.git (fetch)origin  https://github.com/ElysiaFollower/hexo-theme-redefine.git (push)upstream        https://github.com/EvanNotFound/hexo-theme-redefine.git (fetch)upstream        https://github.com/EvanNotFound/hexo-theme-redefine.git (push)\n这是在下fork的一个仓库，这里的origin仓库就是git clone时自动产生的，而upstream仓库则是在下手动添加的。可以看到，在git remote -v眼里，他俩是同一回事。理解到此为止，所以添加之后只需要像正常一样使用git push和git fetch进行代码管理即可。不过需要注意的是，git fetch命令的’完整版’其实应该是[2][3] :git fetch &lt;远程仓库别名&gt; &lt;分支名&gt;只是git clone的时候，同时生成了一些缺省时的默认配置，但是在管理多个远程仓库的时候还是全部显式声明更加所见即所得。\n\nWhen git fetch is run without specifying what branches and/or tags to fetch on the command line, e.g. git fetch origin or git fetch, remote..fetch values are used as the refspecs—​they specify which refs to fetch and which local refs to update[3].\n\nfetch如此，push也是同理的:git push &lt;remote别名&gt; &lt;branch&gt;\n查看远程仓库相关信息:git remote show &lt;remote别名&gt;\n删除远程仓库：git remote remove &lt;remote别名&gt;\n社区暗语从Fork到Pull Request(PR)不知道大家是否会经常听到PR这个词呀，在下第一次听到这个词的时候，还以为是peer review的缩写(死去的fds和ads课程回忆突然开始攻击我), 但是代入上下文语境总觉得有些违和。后来了解之后才知道在开源共享或者项目管理中，PR往往指的实际上是Pull Request，即拉取请求 —— 您向原项目/仓库的维护者提交请求，建议他们合并你所做的修改在我的观察中，PR实际上可以理解为: 存在一个仓库A，而我们的手里有一个仓库B，这是基于A的延伸，或者基于A的修改，我们在仓库B的git历史中存在某一个分支，就像正常分支合并一样，我们可以通过Pull Request将这个分支合并/merge到仓库A的指定分支上中。 \nPR的好处都有啥PR的意义尤其在于开源贡献。请设想这样一个场景: 阁下在网上冲浪的时候发现了一份写得很好的代码，或者自己在使用某个开源工具的过程中体验到了一些难处，作者一直没有解决这个问题于是你决定自己动手。但是当您兴致满满地git clone完原仓库，幸苦完成了修改之后，自信满满地键入git push origin xxx-new-feature，您发现这个操作失败了，然后您意识到——您根本没有这个仓库的提交权限！若阁下尝试去直接联系仓库的维护者，且不说对方是否能够及时回复，就算回复了，对方可能也会出于不想有人把代码搞得一团糟甚至删库跑路等顾忌而不愿轻易提供写权限。那么难道这就死局了吗？非也非也，聪明的程序员们发明了PR机制来解决这个问题。它有以下几个特点(根据在GitHub上的观察):\n\n即便是对仓库没有写权限的人也可以发起PR\n当阁下发起PR的时候，阁下可以为您的修改写一份声色俱茂的小作文，包括目的、动机、实现方法、效果和测试结果等\n仓库的维护者可以直接查看您的修改(diff形式)，如果认为合适就可以合并到原仓库中，完成合并则等效于真的在原仓库中开了这么一条分支，在开发后合并到特定分支\nPR拥有评论区，您可以和仓库的维护者或者其他人在这里展开充满激情的辩论。如果您认为他人说的有道理，您可以回到自己的分支中修改代码(GitHub的PR会自动更新最新提交内容-2025/8/19)，然后您就会惊奇发现，当时那条PR的commit记录自动延长了，阁下可以马上附上更新说明，在一个连续的历史上下文中完善您的这一次贡献\n\n考虑到PR拥有的这些强大的能力——特别是“小作文”和评论区的设计——实际上为代码审查提供了很大的方便。所以在下肤浅的认为，对于一些大型项目的大型改动来说，即便是对于拥有写权限的开发者来说，学习使用PR也是非常有意义的！虽然网上能搜到的一些协作开发的范式大都是不同人负责不同的分支，开发完了合并。或许这对于较为小型的项目来说更为方便，但是试想那位进行merge的人需要承担多大的心里负担呐。擅自将自己开发完的分支合并到别人的分支或者main分支上，说不定过段时间就会有一位悲惨的程序员发现自己原来能跑的代码跑不了了，于是悄悄地破防了(；´д｀)ゞ。如果联系项目负责人，让Ta进行审查合并，且不说联系过程怪麻烦的，而且整个审查并不是公开透明的，未来出现了问题也增加了维护的难度。看，这些问题通过PR机制都能得到很好的解决ヽ(￣▽￣)ﾉ\nFork实乃何物PR这么有用，那么怎么PR呢。这时候就不得不提Fork了。在下以为Fork+PR的组合拳，已经是进行PR的一种社区规范了。\nFork实际上就是将一个仓库复制到自己的账号下，形成一个复制仓库，这样您就有了这个项目的副本，可以随意进行修改，而不会影响到主项目。\n\n也就是说，除了要发起PR的分支，阁下甚至可以开几条属于自己的草稿分支，并且不用担心遭到同伴亲切的问候“ 您在做神魔(O_o)?? ”\n\n至于怎么发起PR，则只需要在GitHub web端进入Fork形成的仓库副本，在Pull Request栏发起PR请求即可。事实上于在下的印象之中，当已经commit了几条之后，Fork的仓库首页就会贴心地提示您”Compare &amp; pull request”[1]\n小技巧在Fork的仓库中，阁下可以通过git remote add upstream &lt;原仓库URL&gt;来添加一个上游仓库(对应的git操作请参考)，这样您就可以通过git fetch upstream来将原仓库的最新代码拉取到自己的仓库中并通过git merge upstream/branch_name合并了！(￣▽￣)~*这样的话，就算是团队内部进行开发或者修改自己使用中的工具，使用Fork仓库也完全无负担，反而更方便，自由度更高\n\n不仅可以无缝追踪原仓库进度\n还有自己的草稿区\n\n注: 一般使用upstream作为原仓库的别名\n\nCommit Message规范网上很有多关于commits规范的说法，有的显得很复杂。在下以为，重要的还是能不能把事情说清楚(看到阿里在文章中说”建议使用中文（感觉中国人用中文描述问题能更清楚一些）”, 笑罢觉得还是有些事情值得反思的，形式主义没有意义)，所以这里摘些个人最认可也认为是最重要的部分充当一下互联网的记忆叭(￣^￣) [6][7]\n最值得参考的大抵还是Angular规范[8]和后来在它的基础上发展出来的Conventional Commits叭[6]Angular范式：\n&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;&lt;body&gt;&lt;footer&gt;\n作用域scope，body和footer都是可选的；前面几个都挺有用的，相对而言footer的可有可无性略高\n然后类型这边在下稍微糅合了一下，但都是接受度比较高的类型:\n\nfeat: 新功能\nfix: 修复bug\ndocs: 文档\nstyle: 样式修改 —— 主要是代码样式修改，不影响运行的那种[6]\nrefactor: 重构，代码重构，逻辑重构\ntest: 测试, 例如添加、删除、修改代码的测试用例等。\nchore: 用于对非业务性代码进行修改，例如修改构建流程或者工具配置等\nbuild: 用于修改项目构建系统，例如修改依赖库、外部接口或者升级 Node 版本等\nci: 用于修改持续集成流程，例如修改 Travis、Jenkins 等工作流配置；\nperf: 用于优化性能，例如提升代码的性能、减少内存占用等；\nrevert: 用于回滚版本\n\nfooter里面的’BREAKING CHANGE‘是什么？—— 简单来说就是不兼容修改, 本次提交修改后将不兼容之前版本的API或者环境变量。建议写上Before和After，分别表示修改前的和修改后的版本。\n关于怎么写，学理论不如看几个例子感受一下(。-`ω´-) ;取自[8] :\nReferencesCoding Is Fun. 开源贡献：从 Fork 到 Pull Request（PR）. CSDN, 2024 ↩Git. 2.5 Git 基础 - 远程仓库的使用. git-scm, ↩Git. git-fetch - Download objects and refs from another repository. git-scm, 2025 ↩Runner_Jack. git fetch &amp; pull详解. 博客园, 2018 ↩Git. git-pull - Fetch from and integrate with another repository or a local branch. git-scm, 2025 ↩. convertionalcommits. , ↩阿里云开发者​. 如何规范你的Git Commit?. 知乎, 2025 ↩angular. AngularJS Git Commit Message Conventions document. github, ↩","categories":["Tools","code"],"tags":["Git","manual","code management"]},{"title":"重读经典：Word2Vec如何通过“简化”撬动了整个NLP世界？","url":"/2025/08/25/@mikolov2013EfficientEstimationWord/","content":"摘要: 2013年，一篇名为《Efficient Estimation of Word Representations in Vector Space》的论文横空出世，它提出的构成Word2Vec核心思想的CBOW和Skip-gram架构，彻底改变了我们让机器理解语言的方式。本文将带您重温这篇里程碑式的著作，探讨其核心思想：为何一个看似更“简单”的模型，反而能发现语言中令人惊叹的深层结构？\n原文链接：https://arxiv.org/abs/1301.3781\n注：为了无障碍理解这篇论文，阁下可能需要先理解文末 前置知识 中提到的一些基本概念。\n问题的根源：昂贵的“理解”在Word2Vec出现之前，自然语言处理（NLP）领域长期面临一个难题：如何让计算机理解词语之间的关系？传统方法如One-hot编码，将每个词视为一个独立的符号，这使得“国王”和“女王”在模型眼中毫无关联。\n\nas these(notion of words) are represented as indices in a vocabulary\n\n虽然当时先进的神经网络语言模型（NNLM）已经可以通过“分布式表示”（Distributed Representation）将词语学习为低维、稠密的向量（即词向量），并在一定程度上捕捉到语义相似性。但这些模型通常包含一个或多个复杂的非线性隐藏层。这个隐藏层是模型强大表达能力的核心，却也成为了巨大的计算瓶颈。其复杂度，特别是Hidden Layer的计算（N×D×H part），使得在数十亿词级别的海量数据集上训练模型变得不切实际。简单来说，当时的“理解”非常昂贵，以至于我们无法用足够多的数据去“喂养”它。\n\nThe main observation from the previous section was that most of the complexity is caused by the non-linear hidden layer in the model\n\n解决方案： 大道至简的两个模型面对上述瓶颈，Mikolov和他的同事们提出了一个颠覆性的思路：能不能用一个更简单的模型，来换取在更大规模数据上训练的能力？于是，他们大胆地移除了计算成本高昂的非线性隐藏层，从而极大地降低了计算复杂度。答案就是本文提出的两个核心架构：CBOW 和 Skip-gram。基本可以认为，这两个模型就是后来广为人知的Word2Vec。\n\n\nContinuous Bag-of-Words (CBOW)\n\n\n任务：根据上下文（周围的词）来预测中心词。\n做法：将目标词周围N个词的向量从投影矩阵(一个巨大的查询表，每一行即为一个词的向量)中取出，然后直接对它们进行求和平均，形成一个汇总的上下文向量，并用这个向量去预测中心词 。因为忽略了上下文的词序，所以被称为“词袋”（Bag-of-Words）模型。\n特点：多对一的预测，训练速度更快，对高频词效果更好。\n\n\nContinuous Skip-gram\n\n\n任务：与CBOW相反，它根据当前的中心词来预测其上下文。\n做法：将中心词的向量作为输入，去预测其前后一定范围（窗口C）内的多个上下文词语。并且通过将窗口的大小设计为固定范围的随机数，便可实现统计意义上的对远端词语进行更少采样，从而给予近处上下文更高的权重，这或许可以被看作是一种朴素的、非动态学习的“注意力”思想的雏形。\n特点：一对多的预测，训练时间更长，但能学习到更好的低频词表示，在大型语料上表现通常更优。论文的实验结果（如Table 3）也证实了这一点，Skip-gram在语义准确性上以55%对24%的巨大优势超过了CBOW。和CBOW不同，Skip-gram的训练模式可能会有些难以理解，容易产生歧义，具体讲解可见 Skip-gram训练机理\n\n这种简化带来的好处是惊人的。计算复杂度从NNLM的Q = N×D + N×D×H + H×V 急剧下降到CBOW的 Q = N×D + N×log_2(V) 和 Skip-gram的 Q = Cx(D + D×log_2(V))。正是这种效率上的巨大飞跃，使得在一天之内处理完16亿词的语料成为可能 ，也为接下来那个“魔法般”的发现奠定了基础。\n注：这里提到的Q基本上可以理解为模型一次预测所涉及的参数量; \n\nSimilar to [18], to compare different model architectures we define first the computational complexity of a model as the number of parameters that need to be accessed to fully train the model.\n\n注：此外，V到log_2(V)的变化是通过使用词汇的二叉树表示实现的，也就是说这对于NNLM的复杂度优化也是有用的，但是优化完之后，性能瓶颈在NxDxH, 仍然没有消失。\n\nWith binary tree representations of the vocabulary, the number of output units that need to be evaluated can go down to around log2(V ). Thus, most of the complexity is caused by the term N × D × H.\n\n惊人的发现：向量空间中的线性关系如果说高效的模型是这篇论文的“肌肉”，那么它揭示的向量线性关系就是其“灵魂”。论文最令人振奋的发现是，通过上述简单模型在海量数据上训练出的词向量，不仅仅是让“猫”和“狗”在空间中彼此靠近，更是捕捉到了词语之间丰富、可量化的类比关系（Analogy）。\n这种关系可以通过简单的向量代数运算来揭示。最经典的例子莫过于：这个等式石破天惊，它表明词向量空间中蕴含着抽象的语义维度，例如“性别”、“皇室”等。从“国王”的向量中减去“男人”的向量，相当于抽离出“男性”这个概念，保留了“皇室权威”等特征；再加上“女人”的向量，就将这个“皇室权威”的特征赋予了女性，最终得到的向量在空间中将接近“女王”对应的向量。在这个空间中，词向量的距离可以表示语义的相似度。\n为了系统性地验证这一发现，作者团队专门构建了一个包含语义(Semantic)和语法(Syntactic)类比问题的综合测试集Semantic-Syntactic Word Relationship test set。示例：\n\n语义类比: Athens is to Greece as Oslo is to? (Norway)\n语法类比: apparent is to apparently as rapid is to? (rapidly)\n\n注: 在实操上，可以通过上述向量运算得到结果向量，最后在整个词汇表的向量中，寻找与这个结果向量距离最近的词向量，这个词就是模型的答案——论文中使用的距离度量为“余弦距离”。在下以为，若所有词向量都预先进行归一化（使其长度为1），那么计算成本高昂的余弦距离就可以被简单的“向量点积”所替代。在单位球面上，按点积大小排序等同于按欧氏距离排序，但计算效率更高。ps: 单位球面上，\n实验结果（论文中的Table 3, 4; 6）雄辩地证明，CBOW和尤其是Skip-gram模型，在这类任务上的准确率远超当时更复杂的NNLM和RNNLM模型，同时训练成本大幅降低 。准确率：训练成本：\n历史回响：前沿性它被推翻了吗？\n思想从未被推翻，反而成为了基石。 “将词语映射到稠密向量空间中，通过无监督学习捕捉其语义”这一核心思想，已经成为整个现代NLP的奠基性观念之一。\n\n它还是最前沿吗？具体技术已演进，但思想永存。\n\nWord2Vec的局限在于它生成的是静态词向量(训练得到一个固定的词向量查询表)，对于一个token只有一个表示，无法处理一词多义问题（例如”bank”的“银行”和“河岸”两个含义共享同一个向量）。而当代的SOTA（State-of-the-Art）技术，如BERT、GPT等基于Transformer的模型，生成的是动态的、上下文相关的词向量。它们能根据句子语境为同一个词生成不同的表示，极大地提升了语义捕捉的精度。大体上应该可以理解成：输入句子之后，句子中的所有词首先经过一层类似于Word2Vec的初始嵌入，然后在经过注意力机制处理后，再输出每个词最终的词向量\n\n同时，CBOW由于其BOW的模型，缺乏对位置的建模； Skip-gram的位置概念也非常模糊，并未显式建模。\n\n此外，word2vec模型本身并未提供直接获取整个句子向量的有效方法，通常采用的“词向量平均”策略会丢失重要的语序和语法信息。\n\n然而，即便在今天，Word2Vec因其高效、轻量的特性，仍在学术研究和工业界中扮演着重要角色，甚至就连它被诟病的无法应对一词多义的问题有时也是一种优势——因为它可以得到一个稳定的向量训练结果，若要知晓某个词的向量，在向量表(投影矩阵)中直接查询即得。\n\n\n杂谈《Efficient Estimation of Word Representations in Vector Space》是一篇充满“反直觉”智慧的论文。它告诉我们，有时最优雅的解决方案并非来自更复杂的模型，而是来自对问题本质的深刻洞察。通过极致的简化，Word2Vec将计算力从复杂的模型结构中解放出来，投入到对海量数据的学习中，最终发现了语言本身蕴含的、令人着迷的数学之美。这不仅是一次技术的胜利，更是一次思想的胜利。\n一些可能容易迷惑的点：最终产物虽然Word2Vec在训练时的确利用了上下文，但请注意，训练的最终产物是一个固定的查询表（即投影矩阵 W）。训练结束后，无论“bank”出现在什么新的句子中，我们去使用它的词向量时，做的动作仅仅是查表——从矩阵W中把“bank”对应的那一行向量取出来。这个向量是一次性生成、全局固定的，所以它是“静态”的、“上下文无关”的。上下文信息在训练时被“蒸馏”进了这个固定的向量里，但在使用时，新的上下文不起作用。\n实际上这种方式训练出来的是一个既能进行嵌入又能进行预测的模型——正是因为有预测的能力，所以才能计算损失进行反向传播，从而不断调试投影矩阵，进而得到较好的词向量表示，也就得到了一个较好的嵌入模型。这或许也算是这类自监督学习（Self-supervised Learning）模型的训练哲学吧。我们并不真正关心模型预测得有多准，这个“预测任务”只是一个“借口”。我们真正的目标是，在完成这个任务的过程中，强迫模型学习到有意义的中间表示——也就是那个高质量的“投影矩阵”（词向量）\n共享投影层的说法论文在介绍模型架构时提到的共享投影层(using a shared projection matrix)可能会让人感到困惑。在下认为，这里的共享主要指的是对于上下文窗口的不同位置，使用是同一个投影矩阵进行映射；对比的是上下文窗口的每个位置分别维护不同的投影矩阵。除此之外，在CBOW中，作者还做了一个更激进的共享——投影后所处的空间位置也进行共享：输入的N个词向量在查找到之后，将被直接平均（Average）成一个单独的D维向量，空间位置信息被彻底抹除；对比在NNLM中，查到的向量会被拼接成N*D维的长向量\n\nNote that the weight matrix between the input and the projection layer is shared for all word positions in the same way as in the NNLM.\n\nSkip-gram训练机理要理解Skip-gram，首先需要明确其核心精髓——它不追求单个预测的精准度 预测任务只是一个“借口”，我们真正的目标是在这个过程中“锤炼”出优秀的词向量\nCBOW是将多个上下文单词加起来，然后用聚合后的向量一次性去预测中心词，每次训练就聚焦一次：目标词和使用聚合向量预测目标词的概率而Skip-gram, 则从中心词出发，和上下文窗口C中的所有词分别组成一个独立的训练样本对，每次训练就聚焦C次：这一次的目标词和使用中心词向量预测该目标词的概率例如：假设我们的句子是I am a fool, 窗口大小C=1, 当前处理的中心词是a则形成两个独立的训练样本对：\n\na -&gt; am\na -&gt; fool所以模型会训练两次，一次输入a，使用预测出am的概率计算损失；一次输入a，使用预测出fool的概率计算损失在使用层级Softmax的时候损失计算可能没有这么简单，但本质不变在反向传播的时候，不仅会调整输出层的权重，更重要的是也会调整a的词向量，从而不断拉扯a的词向量，让它更符合上下文语境\n\n计算复杂度论文中给出的计算复杂度公式乍一看可能有些吓人。但根据论文定义，它估算的主要是训练时需要访问的参数数量，理解起来并不复杂。下面我们进行简要拆解：\n\nNNLM: Q = N×D + N×D×H + H×V\n\n\nN×D：这部分代表从输入层到投影层的计算。将N个上下文单词（通常是one-hot）通过投影矩阵映射为N个D维向量，再拼接成一个N×D维的向量。因为涉及到了投影矩阵中的N行，所以对应参数量为N×D\nN×D×H：这是计算瓶颈，代表从投影层（N×D维）到非线性隐藏层（H维）的全连接计算。在这篇论文的语境下，应该是默认其所对比的NNLM为仅拥有单层非线性隐藏层的经典模型\nH×V：代表从隐藏层（H维）到巨大无比的输出层（V维，V是词汇表大小）的全连接计算。\n\n\nCBOW: Q = N×D + D×log₂(V)\n\n\nN×D：与NNLM类似，查询N个上下文单词的D维向量。\nD×log₂(V)：模型移除了N×D×H的隐藏层。将N个词向量平均后得到仅仅一个D维向量，直接用它来预测输出。同时，输出层使用层序Softmax（Hierarchical Softmax）这种技巧，将原本D×V的计算量降维打击至D×log₂(V)，因为它只需要在一个二叉树（深度约log₂(V)）上做一系列二分类判断，而不是在全部V个词上计算概率。所以这个技术对于NNLM同样适用，理论上上面NNLM的复杂度也可以优化，但是不影响CBOW和Skip-gram仍然远比它快。\n\n\nSkip-gram: Q = C×(D + D×log₂(V))\n\n\nD：输入只有一个词，查询其D维向量。\nD×log₂(V)：用这个D维输入向量，去预测一个上下文单词。与CBOW一样，也用了层序Softmax优化。\nC×(...)：因为Skip-gram模型需要对一个输入词，预测其周围的C个上下文单词，所以上述过程需要重复C次。\n\n一言以蔽之：Word2Vec的效率革命，主要来自（1）砍掉N×D×H的隐藏层 和（2）用log₂(V)复杂度的层序Softmax替换V复杂度的普通Softmax。\n关于层序Softmax的使用，感兴趣的话可以作者参考引用的这几篇论文\n\nT. Mikolov, A. Deoras, D. Povey, L. Burget, J. Cˇ ernocky ́. Strategies for Training Large Scale Neural Network Language Models, In: Proc. Automatic Speech Recognition and Understanding, 2011.\nA. Mnih, G. Hinton. A Scalable Hierarchical Distributed Language Model. Advances in Neural Information Processing Systems 21, MIT Press, 2009.\nF. Morin, Y. Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS, 2005.\n\n前置知识\n词的表示方法：从One-hot到分布式表示\n\nOne-hot：想象一个词典有10万个词，那么“apple”这个词可能被表示成一个10万维的向量，其中只有对应“apple”的位置是1，其余全是0。这种方式无法体现词与词之间的关系 。\n分布式表示 (Distributed Representation)：与One-hot不同，它用一个稠密的、低维度的向量（例如300维）来表示一个词。向量中的每一个维度都代表了词语的某种潜在特征。这篇论文的核心就是如何高效地学习这种表示。\n\n\n全连接层\n\n一个全连接层包含一个权重矩阵W和一个偏置向量b。当输入一个向量x时，输出为y = Wx + b。\n\n\n神经网络语言模型 (NNLM) 的基本结构\n\n阁下只需要大致了解它通常包含输入层、投影层、隐藏层和输出层。\n其中投影层的概念可能会让人有点陌生，但对于理解本文思路又至关重要，所以这里在下简要介绍基本原理\n您可以把它想象成一个巨大的查询表。输入层的一个词就像一个开关，正好选中表中的某一行。这一行，就是一个低维、稠密的向量，它就是那个词的分布式表示。因此，这个投影层矩阵本身，在训练结束后，就成为了我们最终想要的“词向量词典”。\n\n\n\n\n向量空间与相似度\n\n当词语被表示为向量后，我们就可以在多维空间中计算它们之间的距离（如余弦距离）。距离越近，代表这两个词语的语义或用法越相似 。\n\n\n\nReferences[1] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n","categories":["AI","basis"],"tags":["AI","basis","Word2Vec","paper","impressive","NLP"]}]